{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding The Words - Word Embeddings and the Word2Vec model\n",
    "##### David Miller - March 2019 - [Link to Github][1]\n",
    "---\n",
    "[1]:(https://github.com/millerdw/millerdw.github.io/tree/master/_notebooks/FindingTheWords_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a [previous post](https://millerdw.github.io/Word-Roots-and-Associations-in-Vectorised-NLP/) I looked at how stemming appeared to improve the clustering of articles. This was trying to build on the fact that words with the same stem are most likely related, and by stemming, we provide some of that knowledge to the model.\n",
    "\n",
    "I didn't get as far as I wanted to with this train of thought due to some limitations of clustering by word frequency, and I failed to reach the more interesting topics, such as building true associations between words. \n",
    "\n",
    "\n",
    "\n",
    "So with all of that in mind, for this piece I'm going to look at the following:\n",
    "\n",
    "- Word Associations\n",
    "    + PCA as a means of building word associations\n",
    "- Word Embeddings\n",
    "    + Translating words into 'meaning' vectors\n",
    "- Transfer Learnings\n",
    "    + Using the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataAccess\n",
    "import Preprocessing\n",
    "import Vectors\n",
    "import Clustering\n",
    "import WordClouds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate list of tokenised, stemmed articles with stopwords removed\n",
    "rawArticles = DataAccess.getArticles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate list of tokenised, stemmed articles with stopwords removed\n",
    "articles = Preprocessing.preprocessArticles(rawArticles)\n",
    "stemmedArticles = Preprocessing.stemTexts(articles)\n",
    "meaningfulStemmedArticles = Preprocessing.removeStopwords(stemmedArticles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-bcb9d01a41e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#vectorise articles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvectorisedStemmedArticles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstemmedVocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectoriseCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstemmedArticles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvectorisedArticles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremoveStopwordColumns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorisedStemmedArticles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstemmedVocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\millerdw.github.io\\_notebooks\\FindingTheWords_4\\Vectors.py\u001b[0m in \u001b[0;36mvectoriseCorpus\u001b[1;34m(processedTexts)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabToIndexMap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildVocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessedTexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# convert to 2d numpy array of vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mvectorisedTexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectoriseTexts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabToIndexMap\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprocessedTexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvectorisedTexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\millerdw.github.io\\_notebooks\\FindingTheWords_4\\Vectors.py\u001b[0m in \u001b[0;36mvectoriseTexts\u001b[1;34m(vocabToIndexMap, texts)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvectoriseTexts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabToIndexMap\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvectoriseText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabToIndexMap\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvectoriseCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessedTexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\millerdw.github.io\\_notebooks\\FindingTheWords_4\\Vectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvectoriseTexts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabToIndexMap\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvectoriseText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabToIndexMap\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvectoriseCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessedTexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\millerdw.github.io\\_notebooks\\FindingTheWords_4\\Vectors.py\u001b[0m in \u001b[0;36mvectoriseText\u001b[1;34m(vocabToIndexMap, text)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvectoriseText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabToIndexMap\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# encode words as their index in the vocabulary (e.g. possibly 'aardvark' => 23)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mindexArray\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvocabToIndexMap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;31m# transform each\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mfrequencyVector\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindexArray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabToIndexMap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "#vectorise articles\n",
    "vectorisedStemmedArticles, stemmedVocabulary = Vectors.vectoriseCorpus(stemmedArticles)\n",
    "vectorisedArticles, vocabulary = Vectors.removeStopwordColumns(vectorisedStemmedArticles, stemmedVocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=20\n",
    "G=100\n",
    "articleCentroidIds,centroids,performance = Clustering.kMeansCluster(vectorisedArticles,K,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.array([[i,Clustering.countClusterArticles(meaningfulStemmedArticles,articleCentroidIds,i)] for i in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks=np.array([[1,2,5],[6,7,9],[12,17,18]])\n",
    "WordClouds.plotClusterWordCloudArray(meaningfulStemmedArticles,articleCentroidIds,Ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Associations\n",
    "### PCA on Document Level Word Correlations\n",
    "\n",
    "Problem with the dimensionality required in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorisedArticles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "u, s, vt = svds(vectorisedStemmedArticles, vectorisedStemmedArticles.shape[0]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U S V descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u.shape)\n",
    "print(s.shape)\n",
    "print(vt.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(range(s.shape[0]),s/np.sum(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVD(vectorisedText) :\n",
    "    u, s, vt = svds(vectorisedText, vectorisedText.shape[0]-1)\n",
    "    # find\n",
    "    n=np.argmax(s)\n",
    "    # reverse the n first columns of u\n",
    "    u_filt = u[:, n::-1]\n",
    "    # reverse s\n",
    "    s_filt = s[n::-1]\n",
    "    # reverse the n first rows of vt\n",
    "    vt_filt = vt[n::-1, :]\n",
    "    \n",
    "    return u_filt, s_filt, vt_filt\n",
    "\n",
    "u_filt, s_filt, vt_filt = SVD(vectorisedStemmedArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotSVD(s) :\n",
    "    fig, axes = plt.subplots(1,2,figsize=(12,4))\n",
    "    axes[0].scatter(range(s.shape[0]),s/np.sum(s))\n",
    "    axes[1].scatter(range(s.shape[0]),np.cumsum(s)/np.sum(s))\n",
    "    \n",
    "PlotSVD(s_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### working with less\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevantIndeces = [i for i,v in enumerate(np.cumsum(s_filt)/np.sum(s_filt)) if v<=0.8]\n",
    "relevantIndexMask = np.diag([v<=0.8 for v in np.cumsum(s_filt)/np.sum(s_filt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u_filt.shape)\n",
    "print(s_filt.shape)\n",
    "print(relevantIndexMask.shape)\n",
    "print(vt_filt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorisedNoiseFilteredArticles = np.dot(np.dot(u_filt,np.diag(s_filt)),np.dot(relevantIndexMask,vt_filt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=20\n",
    "G=100\n",
    "nfArticleCentroidIds,nfCentroids,nfPerformance = Clustering.kMeansCluster(vectorisedNoiseFilteredArticles,K,G)\n",
    "np.array([[i,Clustering.countClusterArticles(meaningfulStemmedArticles,nfArticleCentroidIds,i)] for i in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks=np.array([[0,2,3],[5,6,10],[14,15,16]])\n",
    "WordClouds.plotClusterWordCloudArray(meaningfulStemmedArticles,nfArticleCentroidIds,Ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Level Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "vocabulary, vocabToIndexMap = Vectors.buildVocabulary(stemmedArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceCount(text) :\n",
    "    return np.sum([word == \"endofsen\" for word in text])+1\n",
    "\n",
    "def paragraphCount(text) :\n",
    "    return np.sum([word == \"endofpar\" for word in text])+1\n",
    "\n",
    "def wordCount(text) :\n",
    "    return len(text)\n",
    "\n",
    "def sentenceVectorise(text) :\n",
    "    sentenceEnds = [i for i,word in enumerate(text) if word == \"endofsen\"]\n",
    "    sentences = np.array(np.split(text, sentenceEnds))\n",
    "    return Vectors.vectoriseTexts(vocabToIndexMap,sentences)\n",
    "    \n",
    "sentenceLevelVectorisedArticles = [sentenceVectorise(stemmedArticles[i]) for i in range(len(stemmedArticles))]\n",
    "allSentences = np.vstack(sentenceLevelVectorisedArticles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentenceLevelVectorisedArticles))\n",
    "print(allSentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_sen, s_sen, vt_sen = SVD(allSentences)\n",
    "PlotSVD(s_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n",
      "15\n",
      "18\n",
      "['the', 'relic', 'in', 'mexico', 'citi', 'may', 'offer', 'clue', 'to', 'the', 'first', 'ever', 'discoveri', 'of', 'an', 'aztec', 'royal', 'burial', 'a', 'newli', 'discov', 'trove', 'of', 'aztec', 'sacrific', 'could', 'lead', 'archaeologist', 'to', 'an', 'elus', 'aztec', 'emperor', 'tomb', 'endofsen', 'endofpar', 'such', 'a', 'discoveri', 'would', 'mark', 'a', 'first', 'sinc', 'no', 'aztec', 'royal', 'burial', 'has', 'yet', 'been', 'found', 'despit', 'decad', 'of', 'dig', 'endofsen', 'endofpar', 'the', 'sacrifici', 'offerings,', 'includ', 'a', 'rich', 'adorn', 'jaguar', 'dress', 'as', 'a', 'warrior,', 'were', 'found', 'in', 'mexico', 'city,', 'reuter', 'report', 'endofsen', 'endofpar', '\"we', 'have', 'enorm', 'expect', 'right', 'now,\"', 'lead', 'archaeologist', 'leonardo', 'lopez', 'lujan', 'said', 'endofsen', 'endofpar', '\"as', 'we', 'go', 'deeper', 'we', 'think', \"we'll\", 'continu', 'find', 'veri', 'rich', 'objects.\"', 'endofpar', 'discov', 'off', 'the', 'step', 'of', 'the', 'aztec', 'holiest', 'temple,', 'the', 'sacrifici', 'offer', 'also', 'includ', 'a', 'young', 'boy,', 'dress', 'to', 'resembl', 'the', 'aztec', 'war', 'god', 'and', 'solar', 'deity,', 'and', 'a', 'set', 'of', 'flint', 'knive', 'elabor', 'decor', 'with', 'mother', 'of', 'pearl', 'and', 'precious', 'stone', 'endofsen', 'endofpar', 'the', 'offer', 'were', 'deposit', 'by', 'aztec', 'priest', 'over', 'five', 'centuri', 'ago', 'in', 'front', 'of', 'the', 'templ', 'where', 'the', 'earliest', 'histor', 'account', 'describ', 'the', 'final', 'rest', 'place', 'of', 'aztec', 'king', 'endofsen', 'endofpar', 'onli', 'around', 'a', 'tenth', 'of', 'the', 'box', 'has', 'been', 'excav', 'but', 'plenti', 'of', 'artefact', 'have', 'alreadi', 'been', 'uncovered,', 'includ', 'a', 'larg', 'amount', 'of', 'shells,', 'bright', 'red', 'starfish', 'that', 'like', 'repres', 'the', 'wateri', 'underworld', 'the', 'aztec', 'believ', 'the', 'sun', 'travel', 'through', 'at', 'night', 'befor', 'emerg', 'in', 'the', 'east', 'to', 'begin', 'a', 'new', 'day', 'endofsen', 'endofpar', '\"there', 'an', 'enorm', 'amount', 'of', 'coral', 'that', 'block', 'what', 'we', 'can', 'see', 'below,\"', 'said', 'archaeologist', 'miguel', 'baez,', 'part', 'of', 'the', 'excav', 'team', 'endofsen', 'endofpar', 'chronicl', 'detail', 'the', 'burial', 'rite', 'of', 'three', 'aztec', 'kings,', 'all', 'brother', 'who', 'rule', 'from', '1469', 'to', '1502', 'endofsen', 'endofpar', 'accord', 'to', 'these', 'accounts,', 'the', 'ruler', 'ash', 'were', 'deposit', 'with', 'opul', 'offer', 'and', 'the', 'heart', 'of', 'sacrif', 'slave', 'endofsen', 'endofpar', 'in', '2006,', 'a', 'huge', 'monolith', 'of', 'the', 'aztec', 'earth', 'goddess', 'was', 'found', 'nearbi', 'with', 'an', 'inscript', 'correspond', 'to', 'the', 'year', '1502,', 'which', 'is', 'when', 'the', 'empir', 'greatest', 'ruler', 'and', 'the', 'last', 'of', 'the', 'brothers,', 'ahuitzotl,', 'pass', 'away', 'endofsen', 'endofpar', 'elizabeth', 'boone,', 'an', 'ancient', 'mexico', 'specialist', 'at', 'tulan', 'university,', 'note', 'that', 'that', 'the', 'jaguar', 'may', 'repres', 'the', 'king', 'as', 'a', 'fearless', 'warrior', 'endofsen', 'endofpar', '\"you', 'could', 'have', 'ahuitzotl', 'in', 'that', 'box,\"', 'she', 'said', 'endofsen', 'endofpar', 'it', 'come', 'after', 'a', 'weekend', 'of', 'specul', 'about', 'the', 'pm', 'leadership', 'and', 'claim', 'of', 'a', 'plot', 'to', 'oust', 'her', 'endofsen', 'endofpar', 'how', 'much', 'do', 'you', 'know?', 'test', 'your', 'knowledg', 'endofpar', 'sign', 'up', 'for', 'our', 'newslett', 'endofpar']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#i=4\n",
    "i+=1\n",
    "print(wordCount(stemmedArticles[i]))\n",
    "print(sentenceCount(stemmedArticles[i]))\n",
    "print(paragraphCount(stemmedArticles[i]))\n",
    "print(stemmedArticles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revectorise with Word2Vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
