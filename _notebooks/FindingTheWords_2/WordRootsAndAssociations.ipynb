{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding The Words - Word Roots and Associations in Vectorised NLP\n",
    "##### David Miller - August 2018 - [Link to Github](https://github.com/millerdw/millerdw.github.io/tree/master/_notebooks/FindingTheWords_2)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In memory of William Fairhurst, my Grandad (and proofreader) - 250 thousand miles to the moon, 93 million miles to the sun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In a previous post on [simple NLP using R](http://millerdw.github.io/_posts/2018-07-23-RSS and Simple Natural Language Processing.html), we developed an algorithm that used frequency analysis of the vocabulary in different texts to cluster those texts together. We looked at a couple of improvements on this theme, including using w-shingling to compare more complex word combinations rather than just vocabulary.\n",
    "\n",
    "In this post I want to develop a couple of those ideas further, and address a few of the shortcomings of those approaches; namely that they are relatively lightweight, and attempt a rather superficial form of unsupervised learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Roots and Associations\n",
    "An immediate downside to comparing complete words and vocabulary within texts is that your algorithm is at the mercy of the author; idioms, favoured words, spelling *conventions* (think 'colour' vs 'color'), spelling *mistakes*... If an algorithm doesn't know to recognise the similarities between such differences (i.e. hasn't been explicitly coded, or taught, to do so), then these all add up to a lot of noise in the signal you are trying to process. This can be a serious problem in texts that are condensed, and have very few words to go by, such as a news article.\n",
    "\n",
    "This is an important point to consider when you're working with an NLP problem. Do I try to solve it by preprocessing the data before it reaches my algorithm? Do I try an algorithm that's more robust to some of these issues, say focussing on strings of characters rather than complete words? I think both are viable options, depending on what your goals are, but for the purposes of this post, I'm going to focus on the former. In short, because I'm more interested in document-level text comparison, I think a character-level algorithm is likely to be overkill, and added to this I'm a big believer in a plug-and-play approach to programming, whereby the different components of an algorithm can be separated (see *pre*-processing), upgraded, replaced, generally-messed-with, forgotten, and even reintroduced at a later date, *without affecting any code elsewhere in the project*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I'm a big believer in a plug-and-play approach to programming, whereby the different components of an algorithm can be separated (see *pre*-processing), upgraded, replaced, generally-messed-with, forgotten, and even reintroduced at a later date, *without affecting any code elsewhere in the project*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variety is the Spice of Life\n",
    "A lot of work has been published around the idea of cleaning or normalising individual words before they're input into an algorithm. This is often referred to as [stemming or lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) the text, and consists of either simply removing suffices until only the core of a word remains (stemming), or performing a more complex analysis over a large vocabulary in order to group the various forms of a word together (lemmatization).\n",
    "\n",
    "For the purposes of demonstration, I'm going to use the [Porter Stemmer](http://stp.lingfil.uu.se/~marie/undervisning/textanalys16/porter.pdf) algorithm to cleanup our vocabulary. The Porter Stemmer works by applying a set of rules or heuristics in order, to accurately reduce as many words as possible to a 'correct' word stem. However, the English language is rather varied - given its [variety of historical influences](https://www.merriam-webster.com/help/faq-history), and England's more recent history of global trade, colonialism, and various forms of cultural osmosis (see ['pukka'](https://blog.oxforddictionaries.com/2013/06/14/pukka/), ['tattoo'](https://www.tattoo.com/blog/origin-word-tattoo/), ['chit'](https://www.etymonline.com/word/chit)) - which means that such a set of rules will never be perfect. \n",
    "\n",
    "There are lots of different stemming algorithms, but this one is the most widely known, and has the advantage of being published in its own library; [Snowball](http://snowballstem.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *\"We don't just borrow from other languages; English pursues them down alleyways, beats them unconscious, and rifles their pockets for loose grammar.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to work\n",
    "For the purposes of this blog, Python is the programming language of choice. Most of my previous NLP work has been in R, however I'm keen to make use of the wide variety of tools in the Python community, especially libraries such as [Numpy](http://www.numpy.org/), [Plotly](https://plot.ly/), the [Natural Language Toolkit (NLTK)](https://www.nltk.org/index.html), and later, perhaps, [Keras](https://keras.io/) for deep learning. \n",
    "\n",
    "While the Snowball library is included in NLTK, it is also available in a lighter python wrapper called 'PyStemmer'. As with all of these 3rd party libraries, you'll have to install a copy in addition to your Python installation if you're doing this at home. You'll need to open a `cmd` terminal (in Windows) and run `pip install PyStemmer` to install the library, and then import it into the relevant Python script, as usual.\n",
    "\n",
    "Believe it or not, this will also be one of my first attempts to build a Python notebook from scratch, so let me know if you find something that's bad practice or poorly executed - I've been working with C#, a very similar language, for my whole career, and I've had plenty of experience in reading, contemplating, and occasionally fixing other people's work in Python, but I've never had the joy of starting from a blank piece of screen! \n",
    "\n",
    "Anyway, here goes... In the script below, we're going to import the Snowball/PyStemmer library containing the Porter Stemmer algorithm, and apply it to a set of similar-sounding words. Note also that we have the choose the language that we're interested in. This might seem obvious, but it highlights something worth remembering; the algorithm uses a set of fixed rules based on the cases, tenses and plural forms of various words, and these will obviously change depending on the language or dialect used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer Languages available:\t ['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import Stemmer as ps\n",
    "\n",
    "# list available stemmers\n",
    "print('Stemmer Languages available:\\t',ps.algorithms())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Word\t Stemmed Word\n",
      "general \t general\n",
      "generalizing \t general\n",
      "generalization \t general\n",
      "generalise \t generalis\n",
      "generalising \t generalis\n",
      "generalisation \t generalis\n"
     ]
    }
   ],
   "source": [
    "# create stemmer class\n",
    "stemmer = ps.Stemmer('english')\n",
    "\n",
    "# stemmer examples\n",
    "rawWords = ['general','generalizing','generalization','generalise','generalising','generalisation']\n",
    "\n",
    "print('Raw Word\\t Stemmed Word')\n",
    "for w in rawWords :\n",
    "    print(w,'\\t',stemmer.stemWord(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... so it doesn't always work perfectly, see above where it treats the British and Americanised words as having separate word stems. This is because the stemming algorithm uses human-defined rules rather than, say, learning the relationships for itself. Clearly, there are only so many exceptions that can be accounted for, and the specific rules that would stem the British spellings of `generalise` into the word `general` are either too complicated to include or haven't been added yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra] JSON, NewsAPI and building a news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "\n",
    "news = NewsApiClient(api_key='2bd0b9a9d4594be6b0ceaa26d1861165')\n",
    "\n",
    "all_news = []\n",
    "for i in range(1,11):\n",
    "    all_news.append(news.get_everything(sources='bbc-news,the-verge,abc-news,ary news,associated press,wired,aftenposten,bbc news,bild,blasting news,bloomberg,business insider,engadget,google news,the verge',\n",
    "                                        from_param='2019-02-20',\n",
    "                                        to='2019-03-20',\n",
    "                                        language='en',\n",
    "                                        page_size=100,\n",
    "                                        page=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': {'id': 'bbc-news', 'name': 'BBC News'}, 'author': None, 'title': 'England v Czech Republic: Go with gut instinct when choosing country, says Michael Keane', 'description': 'Players should \"go with their instinct\" when deciding which country to play for, England defender Michael Keane tells BBC Radio 5 Live.', 'url': 'https://www.bbc.co.uk/sport/football/47635822', 'urlToImage': 'https://ichef.bbci.co.uk/onesport/cps/624/cpsprodpb/13006/production/_106103877_keane_rex2.jpg', 'publishedAt': '2019-03-20T19:00:15Z', 'content': 'Declan Rice made three non-competitive appearances for the Republic of Ireland before choosing to play for England\\r\\nPlayers should \"go with their instinct\" when deciding which country to play for, England defender Michael Keane has told BBC Radio 5 Live.\\r\\nThe… [+1456 chars]'}\n",
      "['Share this with endofpar ', 'Players should \"go with their instinct\" when deciding which country to play for, England defender Michael Keane has told BBC Radio 5 Live. endofpar ', \"The Everton centre-back played for the Republic of Ireland at youth level, as did West Ham's Declan Rice, who has now declared his allegiance to England. endofpar \", 'Last week, manager Gareth Southgate declared that more than 50% of England under-16s have dual nationality. endofpar ', '\"You\\'ve got to go with where you feel like you belong,\" said Keane. endofpar ', '\"I always thought I belonged with England and that\\'s why I have always dreamed of playing for England.  endofpar ', '\"When I was at Ireland, I wasn\\'t good enough to play for England at that time. I was only young and small and still developing. I had in the back of my head that hopefully one day I could play for England. endofpar ', '\"I can\\'t really give them too much advice; I think you have just got to go on your gut feeling. Obviously you have to see how you\\'re performing week in week out and where you think you could end up in your career.\" endofpar ', \"In-form Crystal Palace defender Aaron Wan-Bissaka is one of those players with dual nationality who has yet to be given a senior cap by Southgate. DR Congo's manager Florent Ibenge told the Independent that he wanted the 21-year-old to commit to them. endofpar \", \"Meanwhile, Southampton keeper Angus Gunn, who has been previously been called up to Southgate's squad, could also play for Scotland. His father Bryan, a former goalkeeper, played for Scotland in the early 1990s. endofpar \", 'England begin their Euro 2020 qualifying campaign against the Czech Republic on Friday followed by a match in Montenegro next Monday. endofpar ', 'Share this with endofpar ', 'Get latest scores and headlines sent straight to your phone, sign-up to our newsletter and learn where to find us on online. endofpar ', 'Find ways to get active endofpar ', 'How to get involved in just about any sport or activity endofpar ', 'Find a club, activity or sport near you endofpar ']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "def scrapeArticleUrl(url):\n",
    "    page = requests.get(url)\n",
    "    soup = bs(page.text, 'html.parser')\n",
    "    \n",
    "    for x in soup(\"script\"): x.decompose()\n",
    "    for x in soup(\"meta\"): x.decompose()\n",
    "    for x in soup(\"link\"): x.decompose()    \n",
    "    for x in soup(\"span\"): x.decompose()\n",
    "    for x in soup(\"header\"): x.decompose()\n",
    "    for x in soup(\"nav\"): x.decompose()\n",
    "    for x in soup(\"li\"): x.decompose()\n",
    "        \n",
    "    #print(soup)\n",
    "    return [ p.get_text()+' endofpar ' for p in soup('p') if len(p.get_text().split(' '))>1]\n",
    "\n",
    "rawArticles=[]\n",
    "for news in all_news:\n",
    "    rawArticles=rawArticles+news['articles']\n",
    "\n",
    "# dict comprehension syntax - similar to usage in f#, or the foreach library in R\n",
    "rawArticles = {i : rawArticles[i] for i in range(len(rawArticles))}\n",
    "\n",
    "print(rawArticles[1])\n",
    "\n",
    "rawContents = scrapeArticleUrl(rawArticles[1]['url'])\n",
    "print(rawContents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Description:\n",
      "Players should \"go with their instinct\" when deciding which country to play for, England defender Michael Keane tells BBC Radio 5 Live.\n",
      "\n",
      "Preprocessed Description:\n",
      "['should', '\"go', 'with', 'their', 'instinct\"', 'when', 'deciding', 'which', 'country', 'to', 'play', 'for,', 'england', 'defender', 'michael', 'keane', 'tells', 'bbc', 'radio']\n",
      "\n",
      "Stemmed Description:\n",
      "['should', '\"go', 'with', 'their', 'instinct\"', 'when', 'decid', 'which', 'countri', 'to', 'play', 'for,', 'england', 'defend', 'michael', 'kean', 'tell', 'bbc', 'radio']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocessArticles(articles) :\n",
    "    reEndOfSentence = re.compile('\\\\. ')\n",
    "    reNonAlphaNumeric = re.compile('\\'s|/\\n/|/\\t/|[\\W]+ | ')\n",
    "    processedArticles = {}\n",
    "    for i,article in articles.items() :\n",
    "        # collect contents\n",
    "        contents = [str(article['description'])+' ']+scrapeArticleUrl(article['url'])\n",
    "        # convert contents into words\n",
    "        words = []\n",
    "        for text in contents:\n",
    "            # covert to lower case\n",
    "            text = text.lower()\n",
    "            # replace full stops with ENDOFSEN\n",
    "            text = reEndOfSentence.sub(' endofsen ',text)\n",
    "            # split text into individual words\n",
    "            newWords = text.split(' ')\n",
    "            # remove remaining non-alphanumerics\n",
    "            newWords = [reNonAlphaNumeric.sub('',word) for word in newWords]\n",
    "            words = words + [word for word in newWords if word!='']\n",
    "\n",
    "        # combine into list of processed articles\n",
    "        processedArticles[i] = words\n",
    "        \n",
    "    return processedArticles\n",
    "\n",
    "def stemText(words) :\n",
    "    return [stemmer.stemWord(word) for word in words]\n",
    "\n",
    "def stemArticles(articles) :\n",
    "    return {i:stemText(words) for i,words in articles.items()}\n",
    "\n",
    "\n",
    "articles = preprocessArticles(rawArticles)\n",
    "stemmedArticles = stemArticles(articles)\n",
    "\n",
    "print('\\nRaw Description:')\n",
    "print(rawArticles[1]['description'])\n",
    "print('\\nPreprocessed Description:')\n",
    "print(articles[1][1:20])\n",
    "print('\\nStemmed Description:')\n",
    "print(stemmedArticles[1][1:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorised Representations\n",
    "\n",
    "In my [previous post](https://millerdw.github.io/RSS-and-Simple-Natural-Language-Processing/), we built an algorithm that looked at the differences between articles in order to cluster them together. In that example, we only needed to generate a difference score between the vocabulary used in two texts, in order to be able to cluster them. This was done using a Jaccard Distance based on the vocabulary and n-grams in the two articles being compared.\n",
    "\n",
    "We didn't have to describe the articles outside of this comparison. This simplified matters for us, but it did limit the scope of the encoding of the articles.\n",
    "\n",
    "An alternative method is to build vectorised representations of the articles. In this example, it is a very simple process, which generates a vector defining the frequency distribution of the vocabulary used within a text, where each element of the vector is the frequency of one word within the article.\n",
    "\n",
    "This is simple enough, however, because the same vector-space needs to be used to represent all articles, there must be one element for each word in the English Language, or at the very least, the vocabulary of the entire corpus! This means that we will be dealing  with an extremely high dimensional vector space, which can cause significant computational overheads, and real statistical problems when the number of examples n is much less than the number of dimensions D.\n",
    "\n",
    "### Too much of a good thing\n",
    "It is worth noting that in the [Jaccard Distance](https://millerdw.github.io/RSS-and-Simple-Natural-Language-Processing/) example, we effectively compared documents in a subspace of the vector space that either one or the other article inhabited. This meant that the calculation of document similarity was much more efficient. While it would be computationally expensive to revectorise our articles at every comparison to reduce the overall vocabulary, we may be able to perform a similar function by clipping the vectors we are interested in. However, one of the main benefits of vectorisation is the ability to group all of your vectors up into matrices, and benefit from the extremely optimised Linear Algebra packages available in python.\n",
    "\n",
    "Finally, because stemming makes similar words match with each other, this should reduce the size of the overall vocabulary a bit, but we will still find that the vectors are in tens of thousands of dimensions.\n",
    "\n",
    "See below for code to build a vocabulary from the corpus, and then vectorise individual texts according to that vocabulary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary:\t(1000, 32938)\n",
      "Stemmed Vocabulary:\t(1000, 26908)\n"
     ]
    }
   ],
   "source": [
    "def buildVocabulary(processedArticles):\n",
    "    # generate list of all vocabulary\n",
    "    vocabulary = []\n",
    "    for i,article in processedArticles.items():\n",
    "        vocabulary = vocabulary + article\n",
    "    \n",
    "    vocabulary = list(set(vocabulary))\n",
    "    vocabulary.sort(key=str)\n",
    "\n",
    "    #return as dictionary\n",
    "    return {i:vocabulary[i] for i in range(len(vocabulary))}\n",
    "\n",
    "def vectoriseText(vocabToIndexMap,article):\n",
    "    # encode words as their index in the vocabulary (e.g. possibly 'aardvark' => 23)\n",
    "    indexArray=[vocabToIndexMap[word] for word in article]\n",
    "    # transform each\n",
    "    frequencyVector=[float(np.sum([i==j for j in indexArray])) for i in range(len(vocabToIndexMap))]\n",
    "    return frequencyVector/np.linalg.norm(frequencyVector)\n",
    "\n",
    "def vectoriseArticles(processedArticles):\n",
    "    # build vocabulary\n",
    "    vocabulary = buildVocabulary(processedArticles)\n",
    "    # create map of word to vocabulary index\n",
    "    vocabToIndexMap={w:i for i,w in vocabulary.items()}\n",
    "    # convert to 2d numpy array of vectors\n",
    "    vectorisedArticles = np.vstack([vectoriseText(vocabToIndexMap,article) for i,article in processedArticles.items()])\n",
    "    return vectorisedArticles, vocabulary\n",
    "    \n",
    "  \n",
    "vectorisedArticles, vocabulary = vectoriseArticles(articles)\n",
    "vectorisedStemmedArticles, stemmedVocabulary = vectoriseArticles(stemmedArticles)\n",
    "\n",
    "print('Original Vocabulary:\\t'+str(vectorisedArticles.shape))\n",
    "print('Stemmed Vocabulary:\\t'+str(vectorisedStemmedArticles.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers above show the size of our matrices. Dimension 1, the number of texts, should equal 1000 in both cases; while dimension 2, the number of words in the vector, will vary depending on the number of the size of the vocabulary used in each case.\n",
    "\n",
    "**As expected, the original vocabulary (untreated) is significantly higher than the stemmed vocabulary.**\n",
    "\n",
    "Now, each of these sets of `vectorisedArticles` is a table, or matrix, with one row for every article, and one column for every word in the vocabulary, the value in each cell, or element, is the number of times the word (column) appears in the article (row).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Extra] Performance Tuning\n",
    "To give a measure of the performance gain made by stemming, which doesn't even consider the value of understanding similarity between similar words, I've included timings of a dot product calculation, the same as used for Cosine Similarity calculations, between all article vector combinations (here as a matrix multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Time, Original Vectors: 2.375\n",
      "Calculation Time, Stemmed Vectors: 1.828125\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def singleCalculationTime(vectorisedArticles):\n",
    "    timeStart=time.process_time()\n",
    "    totalDistances=np.dot(vectorisedArticles,vectorisedArticles.T)\n",
    "    timeEnd=time.process_time()\n",
    "    return (timeEnd-timeStart)\n",
    "\n",
    "print('Calculation Time, Original Vectors: '+str(singleCalculationTime(vectorisedArticles)))\n",
    "\n",
    "print('Calculation Time, Stemmed Vectors: '+str(singleCalculationTime(vectorisedStemmedArticles)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using stemming reduces processing costs by ~20%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are alternative methods for reducing the vocabulary size that focus on eliminating over-used vocabulary, but for every word removed, some information is lost. Below is an example of how winsorising your vocabulary will also improve, performance, however I will not be using this is the clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winsorised Original Vocabulary:\t(1000, 19741)\n",
      "Calculation Time, Winsorised Original Vectors: 1.5625\n",
      "Winsorised Stemmed Vocabulary:\t(1000, 16055)\n",
      "Calculation Time, Winsorised Stemmed Vectors: 1.1875\n"
     ]
    }
   ],
   "source": [
    "def winsoriseWordVectors(vectorisedArticles,vocabulary):\n",
    "    wordFrequency = np.sum(vectorisedArticles,axis=0)\n",
    "    #print(wordFrequency.shape)\n",
    "    #print(np.max(wordFrequency))\n",
    "    #print(np.median(wordFrequency))\n",
    "    wordFrequencyDeciles = np.percentile(wordFrequency,[i for i in range(100)], interpolation='higher')\n",
    "    #print(wordFrequencyDeciles)\n",
    "    columnIndeces=[i for i in range(wordFrequency.shape[0]) if wordFrequency[i]>wordFrequencyDeciles[20] and wordFrequency[i]<=wordFrequencyDeciles[80]]\n",
    "    #print(columnIndeces)\n",
    "    return vectorisedArticles[:,columnIndeces],{i:vocabulary[columnIndeces[i]] for i in range(len(columnIndeces))}\n",
    "\n",
    "winsrVectorArticles,winsrVocabulary = winsoriseWordVectors(vectorisedArticles,vocabulary)\n",
    "winsrVectorStemmedArticles,winsrStemmedVocabulary = winsoriseWordVectors(vectorisedStemmedArticles,stemmedVocabulary)\n",
    "\n",
    "print('Winsorised Original Vocabulary:\\t'+str(winsrVectorArticles.shape))\n",
    "print('Calculation Time, Winsorised Original Vectors: '+str(singleCalculationTime(winsrVectorArticles)))\n",
    "print('Winsorised Stemmed Vocabulary:\\t'+str(winsrVectorStemmedArticles.shape))\n",
    "print('Calculation Time, Winsorised Stemmed Vectors: '+str(singleCalculationTime(winsrVectorStemmedArticles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorised k-Means\n",
    "\n",
    "Below I've implemented a more regular version of the k-means clustering algorithm compared to that in my previous post. This uses the vectorised articles and a measure of [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to build the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(vectorA,vectorB):\n",
    "    vectorA = [float(element) for element in vectorA]\n",
    "    vectorB = [float(element) for element in vectorB]\n",
    "    return np.dot(vectorA,vectorB)/(np.linalg.norm(vectorA)*np.linalg.norm(vectorB))\n",
    "\n",
    "def centroidDistances(centroids,article):\n",
    "    return { k:cosineSimilarity(centroid,article) for k,centroid in centroids.items() }\n",
    "\n",
    "def generateCentroid(vectorisedArticles):\n",
    "    randId=np.random.randint(vectorisedArticles.shape[0], size=1)\n",
    "    return vectorisedArticles[randId]\n",
    "\n",
    "def kMeansCluster(vectorisedArticles,K,G):\n",
    "\n",
    "    centroids = np.vstack([ generateCentroid(vectorisedArticles) for k in range(K) ])\n",
    "    articleCentroidIds = []\n",
    "    performance=[]\n",
    "    g = 0\n",
    "    centroidsChanged=True\n",
    "    while (g<G and centroidsChanged):\n",
    "        # generate simple cosine distance on normalised vectors between all articles and centroids\n",
    "        centroidDistances=1-np.dot(vectorisedArticles,centroids.T)\n",
    "        #if g==0: print(centroidDistances.shape)\n",
    "        #if g==0: print(centroidDistances)\n",
    "\n",
    "        # use numpy argmin to find index (column) of nearest centroid by article (row)\n",
    "        articleCentroidIds=np.argmin(centroidDistances,axis=1)\n",
    "        articleCentroidDistances=np.min(centroidDistances,axis=1)\n",
    "        #if g==0: print(articleCentroidIds.shape)\n",
    "        #if g==0: print(articleCentroidDistances)\n",
    "        #if g==0: print(articleCentroidIds)\n",
    "        \n",
    "        # create new centroids by averaging positions of all article vectors in cluster\n",
    "        newCentroids=[]\n",
    "        for k in range(K):\n",
    "            clusterMembers=vectorisedArticles[[i for i in range(articleCentroidIds.shape[0]) if articleCentroidIds[i]==k],:]\n",
    "            if g==G-1: print(str(k)+'\\t'+str(clusterMembers.shape))\n",
    "            clusterSize = clusterMembers.shape[0]\n",
    "            if clusterSize==0:\n",
    "                newCentroid=generateCentroid(vectorisedArticles)\n",
    "            else:\n",
    "                newCentroid=np.mean(clusterMembers,axis=0)\n",
    "            newCentroids.append(newCentroid)\n",
    "        newCentroids=np.vstack(newCentroids)\n",
    "\n",
    "        # update existing centroids\n",
    "        #print(centroids-newCentroids)\n",
    "        \n",
    "        centroidsChanged = (np.sum(np.diag(np.dot(centroids,newCentroids.T))<1.0)>0)\n",
    "        centroids=newCentroids\n",
    "        g+=1\n",
    "        \n",
    "        interCentroidDistance = np.sum(np.sum(1-np.dot(newCentroids,newCentroids.T)))/(2*K*(K-1))\n",
    "        intraCentroidDistance = np.mean(articleCentroidDistances)\n",
    "        performance.append([g,interCentroidDistance,intraCentroidDistance,intraCentroidDistance/interCentroidDistance])\n",
    "        \n",
    "    print('Last Generation:\\t'+str(g))\n",
    "    return articleCentroidIds,centroids,np.vstack(performance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t(367, 26908)\n",
      "1\t(68, 26908)\n",
      "2\t(1, 26908)\n",
      "3\t(2, 26908)\n",
      "4\t(1, 26908)\n",
      "5\t(130, 26908)\n",
      "6\t(28, 26908)\n",
      "7\t(4, 26908)\n",
      "8\t(8, 26908)\n",
      "9\t(326, 26908)\n",
      "10\t(2, 26908)\n",
      "11\t(6, 26908)\n",
      "12\t(4, 26908)\n",
      "13\t(31, 26908)\n",
      "14\t(3, 26908)\n",
      "15\t(1, 26908)\n",
      "16\t(4, 26908)\n",
      "17\t(12, 26908)\n",
      "18\t(1, 26908)\n",
      "19\t(1, 26908)\n",
      "Last Generation:\t100\n"
     ]
    }
   ],
   "source": [
    "K=20\n",
    "G=100\n",
    "articleCentroidIds,centroids,performance = kMeansCluster(vectorisedStemmedArticles,K,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.        ,  0.26958994,  0.15649223,  0.58048244],\n",
       "       [ 3.        ,  0.27398791,  0.18225335,  0.66518758],\n",
       "       [ 4.        ,  0.32206336,  0.17600968,  0.5465064 ],\n",
       "       [ 5.        ,  0.32889314,  0.20688346,  0.6290294 ],\n",
       "       [ 6.        ,  0.33041174,  0.20312905,  0.61477552],\n",
       "       [ 7.        ,  0.33265559,  0.20081128,  0.60366121],\n",
       "       [ 8.        ,  0.33713284,  0.19980279,  0.59265301],\n",
       "       [ 9.        ,  0.34198893,  0.20012372,  0.58517601],\n",
       "       [10.        ,  0.34291885,  0.20026109,  0.58398974],\n",
       "       [11.        ,  0.34296922,  0.1997434 ,  0.58239453],\n",
       "       [12.        ,  0.34297589,  0.19966257,  0.58214752],\n",
       "       [13.        ,  0.34298869,  0.19963127,  0.58203457],\n",
       "       [14.        ,  0.34302356,  0.19962532,  0.58195805],\n",
       "       [15.        ,  0.34299795,  0.1996156 ,  0.58197316],\n",
       "       [16.        ,  0.34297178,  0.19961334,  0.58201097],\n",
       "       [17.        ,  0.3429811 ,  0.19957092,  0.5818715 ],\n",
       "       [18.        ,  0.34290837,  0.19955817,  0.58195771],\n",
       "       [19.        ,  0.34291658,  0.19953042,  0.58186287],\n",
       "       [20.        ,  0.34290465,  0.19949289,  0.58177365],\n",
       "       [21.        ,  0.34298227,  0.19947395,  0.58158678],\n",
       "       [22.        ,  0.34307008,  0.19946522,  0.58141248],\n",
       "       [23.        ,  0.34312886,  0.19942319,  0.58119038],\n",
       "       [24.        ,  0.34317665,  0.19941869,  0.58109631],\n",
       "       [25.        ,  0.34317853,  0.19943807,  0.58114964],\n",
       "       [26.        ,  0.34317853,  0.1995016 ,  0.58133476],\n",
       "       [27.        ,  0.34317853,  0.1995016 ,  0.58133476],\n",
       "       [28.        ,  0.34317853,  0.1995016 ,  0.58133476],\n",
       "       [29.        ,  0.34317853,  0.1995016 ,  0.58133476],\n",
       "       [30.        ,  0.34317853,  0.1995016 ,  0.58133476]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance[1:30,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['should', '\"go', 'with', 'their', 'instinct\"', 'when', 'deciding', 'which', 'country']\n",
      "['patel', 'will', 'captain', 'birmingham', 'bears', 'in', 'this', 'season', 't20']\n",
      "['full-back', 'kieran', 'tierney', 'is', 'not', 'fit', 'for', 'scotland', 'european']\n",
      "['dillashaw', 'says', 'he', 'has', 'vacated', 'the', 'ufc', 'bantamweight', 'title']\n",
      "['tigers', 'back-row', 'forward', 'tommy', 'reffell', 'extends', 'his', 'contract', 'with']\n",
      "['united', 'midfielder', 'calum', 'butcher', 'says', 'his', 'side', 'still', 'believe']\n",
      "['hooker', 'tom', 'cruse', 'agrees', 'a', 'new', 'undisclosed-length', 'contract', 'extension']\n",
      "['johanna', 'konta', 'says', 'playing', 'three', 'home', 'wta', 'grass-court', 'tournaments']\n",
      "['full-back', 'kieran', 'trippier', 'hopes', 'returning', 'to', 'the', 'england', 'squad']\n",
      "['striker', 'will', 'grigg', 'is', 'ruled', 'out', 'of', 'northern', 'ireland']\n",
      "['warrington', 'is', 'open', 'to', 'a', 'rematch', 'against', 'carl', 'frampton']\n",
      "[\"'currency\", \"artist'\", 'ezeanyika', 'anthony', 'chibuzor', 'found', 'life', 'a', 'struggle']\n",
      "['ex-england', 'seamer', 'harry', 'gurney', 'retires', 'from', 'championship', 'cricket', 'to']\n",
      "['your', 'knowledge', '-', 'do', 'you', 'know', 'which', 'premier', 'league']\n",
      "['united', 'defender', 'patrice', 'evra', 'says', '\"only', 'god', 'can', 'judge']\n",
      "['tokyo', 'olympic', 'and', 'paralympic', 'games', 'are', 'set', 'to', 'revolutionise']\n",
      "['rockets', 'guard', 'james', 'harden', 'becomes', 'the', 'first', 'player', 'in']\n",
      "['adam', 'yates', 'misses', 'out', 'on', 'the', 'tirreno-adriatico', 'title', 'by']\n",
      "['coverage', 'of', 'saturday', 'national', 'league', 'game', 'between', 'barnet', 'and']\n",
      "['world', 'heavyweight', 'champion', 'deontay', 'wilder', 'will', 'face', 'american', 'dominic']\n",
      "['slam', 'winners', 'jonathan', 'davies,', 'ken', 'owens', 'and', 'rob', 'evans']\n",
      "['manager', 'marco', 'silva', 'is', 'fined', '£12,000', 'by', 'the', 'football']\n",
      "['retrospective', 'disciplinary', 'action', 'will', 'be', 'taken', 'against', 'any', 'rangers']\n",
      "['will', 'be', 'without', 'injured', 'trio', 'ethan', 'ampadu,', 'sam', 'vokes']\n",
      "['finn', 'russell', 'has', 'revealed', 'he', 'played', 'in', 'scotland', 'six']\n",
      "['could', 'be', 'without', 'opener', 'dimuth', 'karunaratne', 'this', 'season', 'as']\n",
      "['city', 'fa', 'cup', 'semi-final', 'against', 'brighton', 'will', 'be', 'played']\n",
      "['sign', 'italy', 'centre', 'michele', 'campagnaro', 'from', 'premiership', 'rivals', 'wasps']\n"
     ]
    }
   ],
   "source": [
    "k=6\n",
    "for i in range(articleCentroidIds.shape[0]):\n",
    "    if articleCentroidIds[i]==k:\n",
    "        print(articles[i][1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by manually investigating the clusters, we can see that they're of varying quality. Interestingly, some are clustered explicitly by story, and some are clustered by topic (the above group are all sports news articles). While I'm more interested in explicit story groupings, other forms of clustering are inevitable when using simple word frequency measures.\n",
    "\n",
    "Using encodings that combine inter-word relationships, even simple n-grams, can improve this clustering, but this increases the complexity of the model by orders of magnitude. Assuming that a vocabulary has more than 30,000 words, a simple bi-gram vocabulary could contain 1,000,000,000 elements (although in practice significantly fewer due to rules of grammar).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "So we've taken the crux of the k-means clustering algorithm from my last post, and converted it into a vectorised format. We've also looked at stemming algorithms to help improve the algorithms ability to compare between different texts, even if slightly different wording is used.\n",
    "\n",
    "It's still not particularly powerful, as despite the use of stemming logic it continues to lack the power to understand synonyms or inter-word relationships. \n",
    "\n",
    "The former could be improved by a better encoding of the text into vectors, for example using word embeddings such as Word2Vec. The latter could be improved by looking at ways of combining words, such as using n-grams or encoding individual sentences within each document. Unfortunately these will have to wait for a later post.\n",
    "\n",
    "I hope you've found this post interesting! As always, the source code can be found on my [Github](https://github.com/millerdw/millerdw.github.io/tree/master/_notebooks/FindingTheWords_2) page.\n",
    "\n",
    "Thanks for reading!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
