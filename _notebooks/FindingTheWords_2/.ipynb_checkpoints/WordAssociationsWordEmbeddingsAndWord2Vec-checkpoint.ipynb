{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding The Words - Word Associations, Word Embeddings and the Word2Vec model\n",
    "##### David Miller - August 2018 - [Link to Github](https://github.com/millerdw/millerdw.github.io/tree/master/_notebooks/FindingTheWords_2)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In a previous post on [simple NLP using R](http://millerdw.github.io/_posts/2018-07-23-RSS and Simple Natural Language Processing.html), we developed an algorithm that used frequency analysis of the vocabulary in different texts to cluster those texts together. We looked at a couple of improvements on this theme, including using w-shingling to compare more complex word combinations rather than just vocabulary.\n",
    "\n",
    "In this post I want to develop a couple of those ideas further, and address a few of the shortcomings of those approaches; namely that they are relatively lightweight, and attempt a rather superficial form of unsupervised learning, rather than diving deeper into the meaning of the texts.\n",
    "\n",
    "Finally, I'm going to take a little bit of a dive into the Word2Vec model ...\n",
    "\n",
    "- Word Associations\n",
    "    + Word Roots and Synonyms\n",
    "- Word Embeddings\n",
    "    + Translating words into 'meaning' vectors\n",
    "- Transfer Learnings\n",
    "    + Using the Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Associations\n",
    "An immediate downside to comparing complete words and vocabulary within texts is that your algorithm is at the mercy of the author; idioms, favoured words, spelling *conventions* (think 'colour' vs 'color'), spelling *mistakes*... If an algorithm doesn't know to recognise the similarities between such differences (i.e. hasn't been explicitly coded, or taught, to do so), then these all add up to a lot of noise in the signal you are trying to process. This can be a serious problem in texts that are condensed, and have very few words to go by, such as a news article.\n",
    "\n",
    "This is an important point to consider when you're working with an NLP problem. Do I try to solve it by preprocessing the data before it reaches my algorithm? Do I try an algorithm that's more robust to some of these issues, say focussing on strings of characters rather than complete words? I think both are viable options, depending on what your goals are, but for the purposes of this post, I'm going to focus on the former. In short, because I'm more interested in document-level text comparison, I think a character-level algorithm is likely to be overkill, and added to this I'm a big believer in a plug-and-play approach to programming, whereby the different components of an algorithm can be separated (see *pre*-processing), upgraded, replaced, generally-messed-with, forgotten, and even reintroduced at a later date, *without affecting any code elsewhere in the project*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I'm a big believer in a plug-and-play approach to programming, whereby the different components of an algorithm can be separated (see *pre*-processing), upgraded, replaced, generally-messed-with, forgotten, and even reintroduced at a later date, *without affecting any code elsewhere in the project*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variety is the Spice of Life\n",
    "A lot of work has been published around the idea of cleaning or normalising individual words before they're input into an algorithm. This is often referred to as [stemming or lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) the text, and consists of either simply removing suffices until only the core of a word remains (stemming), or performing a more complex analysis over a large vocabulary in order to group the various forms of a word together (lemmatization).\n",
    "\n",
    "\n",
    "Examples...\n",
    "\n",
    "\n",
    "For the purposes of demonstration, I'm going to use the [Porter Stemmer](http://stp.lingfil.uu.se/~marie/undervisning/textanalys16/porter.pdf) algorithm to cleanup our vocabulary. The Porter Stemmer works by applying a set of rules or heuristics in order, to accurately reduce as many words as possible to a 'correct' word stem. However, the English language is rather varied - given its [variety of historical influences](https://www.merriam-webster.com/help/faq-history), and England's more recent history of global trade, colonialism, and various forms of cultural osmosis (see ['pukka'](https://blog.oxforddictionaries.com/2013/06/14/pukka/), ['tattoo'](https://www.tattoo.com/blog/origin-word-tattoo/), ['chit'](https://www.etymonline.com/word/chit)) - which means that such a set of rules will never be perfect. \n",
    "\n",
    "There are lots of different stemming algorithms, but this one is the most widely known, and has the advantage of being published in its own library; [Snowball](http://snowballstem.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *\"We don't just borrow from other languages; English pursues them down alleyways, beats them unconscious, and rifles their pockets for loose grammar.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to work\n",
    "For the purposes of this blog, Python is the programming language of choice. Most of my previous NLP work has been in R, so this will be a first for me. The main reason for my doing this is to make use of the wide variety of tools in the Python community, especially libraries such as [Numpy](http://www.numpy.org/), [Plotly](https://plot.ly/), the [Natural Language Toolkit (NLTK)](https://www.nltk.org/index.html), and later, perhaps, [Keras](https://keras.io/) for deep learning. \n",
    "\n",
    "While the Snowball library is included in NLTK, it is also available in a lighter python wrapper called 'PyStemmer'. As with all of these 3rd party libraries, you'll have to install a copy in addition to your Python installation if you're doing this at home. You'll need to open a `cmd` terminal (in Windows) and run `pip install PyStemmer` to install the library, and then import it into the relevant Python script, as usual.\n",
    "\n",
    "Believe it or not, this will also be my first attempt to build a functioning Python script of my own, so let me know if you find something that's bad practice or poorly executed - I've been working with C#, a very similar language, for my whole career, and I've had plenty of experience in reading, contemplating, and occasionally fixing other people's work in Python, but I've never had the joy of starting from scratch! \n",
    "\n",
    "Anyway, here goes... In the script below, we're going to import the Snowball/PyStemmer library containing the Porter Stemmer algorithm, and apply it to a set of similar-sounding words. Note also that we have the choose the language that we're interested in. This might seem obvious, but it highlights something worth remembering; the algorithm uses a set of fixed rules based on the cases, tenses and plural forms of various words, and these will obviously change depending on the language or dialect used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer Languages available:\t ['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import Stemmer as ps\n",
    "\n",
    "# list available stemmers\n",
    "print('Stemmer Languages available:\\t',ps.algorithms())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Word\t Stemmed Word\n",
      "general \t general\n",
      "generalizing \t general\n",
      "generalization \t general\n",
      "generalise \t generalis\n",
      "generalising \t generalis\n",
      "generalisation \t generalis\n"
     ]
    }
   ],
   "source": [
    "# create stemmer class\n",
    "stemmer = ps.Stemmer('english')\n",
    "\n",
    "# stemmer examples\n",
    "rawWords = ['general','generalizing','generalization','generalise','generalising','generalisation']\n",
    "\n",
    "print('Raw Word\\t Stemmed Word')\n",
    "for w in rawWords :\n",
    "    print(w,'\\t',stemmer.stemWord(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... so it doesn't always work perfectly, because it uses human-defined rules rather than leaarnign for itsle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra] JSON, NewsAPI and building a news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "\n",
    "news = NewsApiClient(api_key='2bd0b9a9d4594be6b0ceaa26d1861165')\n",
    "\n",
    "all_news = []\n",
    "for i in range(1,11):\n",
    "    all_news.append(news.get_everything(sources='bbc-news,the-verge,abc-news,ary news,associated press,wired,aftenposten,bbc news,bild,blasting news,bloomberg,business insider,engadget,google news,the verge',\n",
    "                                        from_param='2018-08-01',\n",
    "                                        to='2018-08-14',\n",
    "                                        language='en',\n",
    "                                        page_size=100,\n",
    "                                        page=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n endofpar ', \"\\nThe former deputy head of Colombia's intelligence agency was convicted Tuesday in the 1999 killing of beloved satirist Jaime Garzon and sentenced to 30 years in prison. endofpar \", \"\\nJose Miguel Narvaez's conviction for aggravated homicide came a day after Colombians commemorated the 19th anniversary of Garzon's slaying. The killing shocked Colombians who looked to Garzon as a trusted intermediary with leftist rebels at a time of rising political violence. endofpar \", \"\\nColombia's top administrative court in 2016 found the state responsible for Garzon's killing. The now-disbanded DAS intelligence agency carried out illegal wiretaps on Garzon, and Narvaez shared the information with paramilitary leaders opposed to the humorist's peace efforts. endofpar \", \"\\nGarzon's family celebrated the conviction but said they would appeal to have the killing classified as a crime against humanity. endofpar \"]\n",
      "1000\n",
      "{'source': {'id': 'abc-news', 'name': 'ABC News'}, 'author': 'The Associated Press', 'title': \"Ex-Colombia spy boss convicted in satirist's 1999 killing\", 'description': \"The former deputy head of Colombia's intelligence agency has been convicted in the 1999 killing of beloved satirist Jaime Garzon and sentenced to 30 years in prison\", 'url': 'https://abcnews.go.com/International/wireStory/colombia-spy-boss-convicted-satirists-1999-killing-57181739', 'urlToImage': None, 'publishedAt': '2018-08-14T23:57:07Z'}\n",
      "\n",
      "The former deputy head of Colombia's intelligence agency was convicted Tuesday in the 1999 killing of beloved satirist Jaime Garzon and sentenced to 30 years in prison. endofpar \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "def scrapeArticleUrl(url):\n",
    "    page = requests.get(url)\n",
    "    soup = bs(page.text, 'html.parser')\n",
    "    return [ p.get_text()+' endofpar ' for p in soup('p') ]\n",
    "\n",
    "rawArticles=[]\n",
    "for news in all_news:\n",
    "    rawArticles=rawArticles+news['articles']\n",
    "\n",
    "# dict comprehension syntax - similar to usage in f#, or the foreach library in R\n",
    "rawArticles = {i : rawArticles[i] for i in range(len(rawArticles))}\n",
    "\n",
    "#\n",
    "rawContents = scrapeArticleUrl(rawArticles[1]['url'])\n",
    "#rawContents = {i : scrapeArticleUrl(rawArticles[i]['url']) for i in range(len(rawArticles))}\n",
    "print(rawContents)\n",
    "print(len(rawArticles))\n",
    "print(rawArticles[1])\n",
    "print(rawContents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Description:\n",
      "The former deputy head of Colombia's intelligence agency has been convicted in the 1999 killing of beloved satirist Jaime Garzon and sentenced to 30 years in prison\n",
      "\n",
      "Preprocessed Description:\n",
      "['the', 'former', 'deputy', 'head', 'of', 'colombia', 'intelligence', 'agency', 'has', 'been', 'convicted', 'in', 'the', '1999', 'killing', 'of', 'beloved', 'satirist', 'jaime', 'garzon', 'and', 'sentenced', 'to', '30', 'years', 'in', 'prison', '\\n', 'endofpar', '\\nthe', 'former', 'deputy', 'head', 'of', 'colombia', 'intelligence', 'agency', 'was', 'convicted', 'tuesday', 'in', 'the', '1999', 'killing', 'of', 'beloved', 'satirist', 'jaime', 'garzon', 'and', 'sentenced', 'to', '30', 'years', 'in', 'prison', 'endofsen', 'endofpar', '\\njose', 'miguel', 'narvaez', 'conviction', 'for', 'aggravated', 'homicide', 'came', 'a', 'day', 'after', 'colombians', 'commemorated', 'the', '19th', 'anniversary', 'of', 'garzon', 'slaying', 'endofsen', 'the', 'killing', 'shocked', 'colombians', 'who', 'looked', 'to', 'garzon', 'as', 'a', 'trusted', 'intermediary', 'with', 'leftist', 'rebels', 'at', 'a', 'time', 'of', 'rising', 'political', 'violence', 'endofsen', 'endofpar', '\\ncolombia', 'top', 'administrative', 'court', 'in', '2016', 'found', 'the', 'state', 'responsible', 'for', 'garzon', 'killing', 'endofsen', 'the', 'now-disbanded', 'das', 'intelligence', 'agency', 'carried', 'out', 'illegal', 'wiretaps', 'on', 'garzon,', 'and', 'narvaez', 'shared', 'the', 'information', 'with', 'paramilitary', 'leaders', 'opposed', 'to', 'the', 'humorist', 'peace', 'efforts', 'endofsen', 'endofpar', '\\ngarzon', 'family', 'celebrated', 'the', 'conviction', 'but', 'said', 'they', 'would', 'appeal', 'to', 'have', 'the', 'killing', 'classified', 'as', 'a', 'crime', 'against', 'humanity', 'endofsen', 'endofpar']\n",
      "\n",
      "Stemmed Description:\n",
      "['the', 'former', 'deputi', 'head', 'of', 'colombia', 'intellig', 'agenc', 'has', 'been', 'convict', 'in', 'the', '1999', 'kill', 'of', 'belov', 'satirist', 'jaim', 'garzon', 'and', 'sentenc', 'to', '30', 'year', 'in', 'prison', '\\n', 'endofpar', '\\nthe', 'former', 'deputi', 'head', 'of', 'colombia', 'intellig', 'agenc', 'was', 'convict', 'tuesday', 'in', 'the', '1999', 'kill', 'of', 'belov', 'satirist', 'jaim', 'garzon', 'and', 'sentenc', 'to', '30', 'year', 'in', 'prison', 'endofsen', 'endofpar', '\\njose', 'miguel', 'narvaez', 'convict', 'for', 'aggrav', 'homicid', 'came', 'a', 'day', 'after', 'colombian', 'commemor', 'the', '19th', 'anniversari', 'of', 'garzon', 'slay', 'endofsen', 'the', 'kill', 'shock', 'colombian', 'who', 'look', 'to', 'garzon', 'as', 'a', 'trust', 'intermediari', 'with', 'leftist', 'rebel', 'at', 'a', 'time', 'of', 'rise', 'polit', 'violenc', 'endofsen', 'endofpar', '\\ncolombia', 'top', 'administr', 'court', 'in', '2016', 'found', 'the', 'state', 'respons', 'for', 'garzon', 'kill', 'endofsen', 'the', 'now-disband', 'das', 'intellig', 'agenc', 'carri', 'out', 'illeg', 'wiretap', 'on', 'garzon,', 'and', 'narvaez', 'share', 'the', 'inform', 'with', 'paramilitari', 'leader', 'oppos', 'to', 'the', 'humorist', 'peac', 'effort', 'endofsen', 'endofpar', '\\ngarzon', 'famili', 'celebr', 'the', 'convict', 'but', 'said', 'they', 'would', 'appeal', 'to', 'have', 'the', 'kill', 'classifi', 'as', 'a', 'crime', 'against', 'human', 'endofsen', 'endofpar']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocessArticles(articles) :\n",
    "    reEndOfSentence = re.compile('\\\\. ')\n",
    "    reNonAlphaNumeric = re.compile('\\'s|/\\n/|/\\t/|[\\W]+ | ')\n",
    "    processedArticles = {}\n",
    "    for i,article in articles.items() :\n",
    "        # collect contents\n",
    "        contents = [str(article['description'])+' ']+scrapeArticleUrl(article['url'])\n",
    "        # convert contents into words\n",
    "        words = []\n",
    "        for text in contents:\n",
    "            # covert to lower case\n",
    "            text = text.lower()\n",
    "            # replace full stops with ENDOFSEN\n",
    "            text = reEndOfSentence.sub(' endofsen ',text)\n",
    "            # split text into individual words\n",
    "            newWords = text.split(' ')\n",
    "            # remove remaining non-alphanumerics\n",
    "            newWords = [reNonAlphaNumeric.sub('',word) for word in newWords]\n",
    "            words = words + [word for word in newWords if word!='']\n",
    "\n",
    "        # combine into list of processed articles\n",
    "        processedArticles[i] = words\n",
    "        \n",
    "    return processedArticles\n",
    "\n",
    "def stemText(words) :\n",
    "    return [stemmer.stemWord(word) for word in words]\n",
    "\n",
    "def stemArticles(articles) :\n",
    "    return {i:stemText(words) for i,words in articles.items()}\n",
    "\n",
    "\n",
    "articles = preprocessArticles(rawArticles)\n",
    "stemmedArticles = stemArticles(articles)\n",
    "\n",
    "print('\\nRaw Description:')\n",
    "print(rawArticles[1]['description'])\n",
    "print('\\nPreprocessed Description:')\n",
    "print(articles[1])\n",
    "print('\\nStemmed Description:')\n",
    "print(stemmedArticles[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorised Implementations\n",
    "\n",
    "very simple, vector defines whole text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary:\t(1000, 31111)\n",
      "Stemmed Vocabulary:\t(1000, 25287)\n"
     ]
    }
   ],
   "source": [
    "def buildVocabulary(processedArticles):\n",
    "    # generate list of all vocabulary\n",
    "    vocabulary = []\n",
    "    for i,article in processedArticles.items():\n",
    "        vocabulary = vocabulary + article\n",
    "    \n",
    "    vocabulary = list(set(vocabulary))\n",
    "    vocabulary.sort(key=str)\n",
    "\n",
    "    #return as dictionary\n",
    "    return {i:vocabulary[i] for i in range(len(vocabulary))}\n",
    "\n",
    "def vectoriseText(vocabToIndexMap,article):\n",
    "    # encode words as their index in the vocabulary (e.g. possibly 'aardvark' => 23)\n",
    "    indexArray=[vocabToIndexMap[word] for word in article]\n",
    "    # transform each\n",
    "    frequencyVector=[float(np.sum([i==j for j in indexArray])) for i in range(len(vocabToIndexMap))]\n",
    "    return frequencyVector/np.linalg.norm(frequencyVector)\n",
    "\n",
    "def vectoriseArticles(processedArticles):\n",
    "    # build vocabulary\n",
    "    vocabulary = buildVocabulary(processedArticles)\n",
    "    # create map of word to vocabulary index\n",
    "    vocabToIndexMap={w:i for i,w in vocabulary.items()}\n",
    "    # convert to 2d numpy array of vectors\n",
    "    vectorisedArticles = np.vstack([vectoriseText(vocabToIndexMap,article) for i,article in processedArticles.items()])\n",
    "    return vectorisedArticles, vocabulary\n",
    "    \n",
    "  \n",
    "vectorisedArticles, vocabulary = vectoriseArticles(articles)\n",
    "vectorisedStemmedArticles, stemmedVocabulary = vectoriseArticles(stemmedArticles)\n",
    "\n",
    "print('Original Vocabulary:\\t'+str(vectorisedArticles.shape))\n",
    "print('Stemmed Vocabulary:\\t'+str(vectorisedStemmedArticles.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers above show the size of our matrices. Dimension 1, the number or rows, should equal roughly 1000 in both cases; while dimension 2, the number of columns, will vary depending on the number of the size of the vocabulary used in each case. Because stemming makes similar words match with each other, this should reduce the size of the vocabulary.\n",
    "\n",
    "**As expected, the original vocabulary (untreated) is significantly higher than the stemmed vocabulary.**\n",
    "\n",
    "Now, each of these sets of `vectorisedArticles` is a table, or matrix, with one row for every article, and one column for every word in the vocabulary, the value in each cell, or element, is the number of times the word (column) appears in the article (row).\n",
    "\n",
    "This gives us an n-dimensional map of where ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Time, Original Vectors: 2.203125\n",
      "Calculation Time, Stemmed Vectors: 1.75\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def singleCalculationTime(vectorisedArticles):\n",
    "    timeStart=time.process_time()\n",
    "    totalDistances=np.dot(vectorisedArticles,vectorisedArticles.T)\n",
    "    timeEnd=time.process_time()\n",
    "    return (timeEnd-timeStart)\n",
    "\n",
    "print('Calculation Time, Original Vectors: '+str(singleCalculationTime(vectorisedArticles)))\n",
    "\n",
    "print('Calculation Time, Stemmed Vectors: '+str(singleCalculationTime(vectorisedStemmedArticles)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winsorised Original Vocabulary:\t(1000, 18643)\n",
      "Calculation Time, Winsorised Original Vectors: 1.25\n",
      "Winsorised Stemmed Vocabulary:\t(1000, 15184)\n",
      "Calculation Time, Winsorised Stemmed Vectors: 1.0625\n"
     ]
    }
   ],
   "source": [
    "def winsoriseWordVectors(vectorisedArticles,vocabulary):\n",
    "    wordFrequency = np.sum(vectorisedArticles,axis=0)\n",
    "    #print(wordFrequency.shape)\n",
    "    #print(np.max(wordFrequency))\n",
    "    #print(np.median(wordFrequency))\n",
    "    wordFrequencyDeciles = np.percentile(wordFrequency,[i for i in range(100)], interpolation='higher')\n",
    "    #print(wordFrequencyDeciles)\n",
    "    columnIndeces=[i for i in range(wordFrequency.shape[0]) if wordFrequency[i]>wordFrequencyDeciles[20] and wordFrequency[i]<=wordFrequencyDeciles[80]]\n",
    "    #print(columnIndeces)\n",
    "    return vectorisedArticles[:,columnIndeces],{i:vocabulary[columnIndeces[i]] for i in range(len(columnIndeces))}\n",
    "\n",
    "winsrVectorArticles,winsrVocabulary = winsoriseWordVectors(vectorisedArticles,vocabulary)\n",
    "winsrVectorStemmedArticles,winsrStemmedVocabulary = winsoriseWordVectors(vectorisedStemmedArticles,stemmedVocabulary)\n",
    "\n",
    "print('Winsorised Original Vocabulary:\\t'+str(winsrVectorArticles.shape))\n",
    "print('Calculation Time, Winsorised Original Vectors: '+str(singleCalculationTime(winsrVectorArticles)))\n",
    "print('Winsorised Stemmed Vocabulary:\\t'+str(winsrVectorStemmedArticles.shape))\n",
    "print('Calculation Time, Winsorised Stemmed Vectors: '+str(singleCalculationTime(winsrVectorStemmedArticles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorised k-Means\n",
    "\n",
    "\n",
    "cosine similarity, automatically normalises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(vectorA,vectorB):\n",
    "    vectorA = [float(element) for element in vectorA]\n",
    "    vectorB = [float(element) for element in vectorB]\n",
    "    return np.dot(vectorA,vectorB)/(np.linalg.norm(vectorA)*np.linalg.norm(vectorB))\n",
    "\n",
    "def centroidDistances(centroids,article):\n",
    "    return { k:cosineSimilarity(centroid,article) for k,centroid in centroids.items() }\n",
    "\n",
    "def generateCentroid(vectorisedArticles):\n",
    "    randId=np.random.randint(vectorisedArticles.shape[0], size=1)\n",
    "    return vectorisedArticles[randId]\n",
    "\n",
    "def kMeansCluster(vectorisedArticles,K,G):\n",
    "\n",
    "    centroids = np.vstack([ generateCentroid(vectorisedArticles) for k in range(K) ])\n",
    "    articleCentroidIds = []\n",
    "    performance=[]\n",
    "    g = 0\n",
    "    centroidsChanged=True\n",
    "    while (g<G and centroidsChanged):\n",
    "        # generate simple cosine distance on normalised vectors between all articles and centroids\n",
    "        centroidDistances=1-np.dot(vectorisedArticles,centroids.T)\n",
    "        if g==0: print(centroidDistances.shape)\n",
    "        #if g==0: print(centroidDistances)\n",
    "\n",
    "        # use numpy argmin to find index (column) of nearest centroid by article (row)\n",
    "        articleCentroidIds=np.argmin(centroidDistances,axis=1)\n",
    "        articleCentroidDistances=np.min(centroidDistances,axis=1)\n",
    "        if g==0: print(articleCentroidIds.shape)\n",
    "        #if g==0: print(articleCentroidDistances)\n",
    "        #if g==0: print(articleCentroidIds)\n",
    "        \n",
    "        # create new centroids by averaging positions of all article vectors in cluster\n",
    "        newCentroids=[]\n",
    "        for k in range(K):\n",
    "            clusterMembers=vectorisedArticles[[i for i in range(articleCentroidIds.shape[0]) if articleCentroidIds[i]==k],:]\n",
    "            if g==0 or g==G-1: print(str(k)+'\\t'+str(clusterMembers.shape))\n",
    "            clusterSize = clusterMembers.shape[0]\n",
    "            if clusterSize==0:\n",
    "                newCentroid=generateCentroid(vectorisedArticles)\n",
    "            else:\n",
    "                newCentroid=np.mean(clusterMembers,axis=0)\n",
    "            newCentroids.append(newCentroid)\n",
    "        newCentroids=np.vstack(newCentroids)\n",
    "\n",
    "        # update existing centroids\n",
    "        #print(centroids-newCentroids)\n",
    "        \n",
    "        centroidsChanged = (np.sum(np.diag(np.dot(centroids,newCentroids.T))<1.0)>0)\n",
    "        centroids=newCentroids\n",
    "        g+=1\n",
    "        \n",
    "        interCentroidDistance = np.sum(np.sum(1-np.dot(newCentroids,newCentroids.T)))/(2*K*(K-1))\n",
    "        intraCentroidDistance = np.mean(articleCentroidDistances)\n",
    "        performance.append([g,interCentroidDistance,intraCentroidDistance,intraCentroidDistance/interCentroidDistance])\n",
    "        \n",
    "    print('Last Generation:\\t'+str(g))\n",
    "    return articleCentroidIds,centroids,np.vstack(performance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20)\n",
      "(1000,)\n",
      "0\t(691, 18643)\n",
      "1\t(11, 18643)\n",
      "2\t(0, 18643)\n",
      "3\t(40, 18643)\n",
      "4\t(41, 18643)\n",
      "5\t(0, 18643)\n",
      "6\t(1, 18643)\n",
      "7\t(0, 18643)\n",
      "8\t(23, 18643)\n",
      "9\t(27, 18643)\n",
      "10\t(17, 18643)\n",
      "11\t(0, 18643)\n",
      "12\t(24, 18643)\n",
      "13\t(19, 18643)\n",
      "14\t(1, 18643)\n",
      "15\t(46, 18643)\n",
      "16\t(36, 18643)\n",
      "17\t(0, 18643)\n",
      "18\t(23, 18643)\n",
      "19\t(0, 18643)\n",
      "0\t(407, 18643)\n",
      "1\t(36, 18643)\n",
      "2\t(25, 18643)\n",
      "3\t(49, 18643)\n",
      "4\t(49, 18643)\n",
      "5\t(13, 18643)\n",
      "6\t(1, 18643)\n",
      "7\t(44, 18643)\n",
      "8\t(50, 18643)\n",
      "9\t(43, 18643)\n",
      "10\t(41, 18643)\n",
      "11\t(18, 18643)\n",
      "12\t(33, 18643)\n",
      "13\t(34, 18643)\n",
      "14\t(1, 18643)\n",
      "15\t(44, 18643)\n",
      "16\t(47, 18643)\n",
      "17\t(12, 18643)\n",
      "18\t(39, 18643)\n",
      "19\t(14, 18643)\n",
      "Last Generation:\t100\n"
     ]
    }
   ],
   "source": [
    "K=20\n",
    "G=100\n",
    "articleCentroidIds,centroids,performance = kMeansCluster(winsrVectorArticles,K,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.           0.52606408   0.99954317   1.9000407 ]\n",
      " [  2.           0.52621891   0.99951547   1.89942903]\n",
      " [  3.           0.52624217   0.99955417   1.8994186 ]\n",
      " [  4.           0.52624471   0.9995667    1.89943324]\n",
      " [  5.           0.52624531   0.99956266   1.89942342]\n",
      " [  6.           0.52624553   0.99956227   1.89942186]\n",
      " [  7.           0.52624553   0.99956285   1.89942296]\n",
      " [  8.           0.52624553   0.99956285   1.89942296]\n",
      " [  9.           0.52624553   0.99956285   1.89942296]\n",
      " [ 10.           0.52624553   0.99956285   1.89942296]\n",
      " [ 11.           0.52624553   0.99956285   1.89942296]\n",
      " [ 12.           0.52624553   0.99956285   1.89942296]\n",
      " [ 13.           0.52624553   0.99956285   1.89942296]\n",
      " [ 14.           0.52624553   0.99956285   1.89942296]\n",
      " [ 15.           0.52624553   0.99956285   1.89942296]\n",
      " [ 16.           0.52624553   0.99956285   1.89942296]\n",
      " [ 17.           0.52624553   0.99956285   1.89942296]\n",
      " [ 18.           0.52624553   0.99956285   1.89942296]\n",
      " [ 19.           0.52624553   0.99956285   1.89942296]\n",
      " [ 20.           0.52624553   0.99956285   1.89942296]\n",
      " [ 21.           0.52624553   0.99956285   1.89942296]\n",
      " [ 22.           0.52624553   0.99956285   1.89942296]\n",
      " [ 23.           0.52624553   0.99956285   1.89942296]\n",
      " [ 24.           0.52624553   0.99956285   1.89942296]\n",
      " [ 25.           0.52624553   0.99956285   1.89942296]\n",
      " [ 26.           0.52624553   0.99956285   1.89942296]\n",
      " [ 27.           0.52624553   0.99956285   1.89942296]\n",
      " [ 28.           0.52624553   0.99956285   1.89942296]\n",
      " [ 29.           0.52624553   0.99956285   1.89942296]\n",
      " [ 30.           0.52624553   0.99956285   1.89942296]\n",
      " [ 31.           0.52624553   0.99956285   1.89942296]\n",
      " [ 32.           0.52624553   0.99956285   1.89942296]\n",
      " [ 33.           0.52624553   0.99956285   1.89942296]\n",
      " [ 34.           0.52624553   0.99956285   1.89942296]\n",
      " [ 35.           0.52624553   0.99956285   1.89942296]\n",
      " [ 36.           0.52624553   0.99956285   1.89942296]\n",
      " [ 37.           0.52624553   0.99956285   1.89942296]\n",
      " [ 38.           0.52624553   0.99956285   1.89942296]\n",
      " [ 39.           0.52624553   0.99956285   1.89942296]\n",
      " [ 40.           0.52624553   0.99956285   1.89942296]\n",
      " [ 41.           0.52624553   0.99956285   1.89942296]\n",
      " [ 42.           0.52624553   0.99956285   1.89942296]\n",
      " [ 43.           0.52624553   0.99956285   1.89942296]\n",
      " [ 44.           0.52624553   0.99956285   1.89942296]\n",
      " [ 45.           0.52624553   0.99956285   1.89942296]\n",
      " [ 46.           0.52624553   0.99956285   1.89942296]\n",
      " [ 47.           0.52624553   0.99956285   1.89942296]\n",
      " [ 48.           0.52624553   0.99956285   1.89942296]\n",
      " [ 49.           0.52624553   0.99956285   1.89942296]\n",
      " [ 50.           0.52624553   0.99956285   1.89942296]\n",
      " [ 51.           0.52624553   0.99956285   1.89942296]\n",
      " [ 52.           0.52624553   0.99956285   1.89942296]\n",
      " [ 53.           0.52624553   0.99956285   1.89942296]\n",
      " [ 54.           0.52624553   0.99956285   1.89942296]\n",
      " [ 55.           0.52624553   0.99956285   1.89942296]\n",
      " [ 56.           0.52624553   0.99956285   1.89942296]\n",
      " [ 57.           0.52624553   0.99956285   1.89942296]\n",
      " [ 58.           0.52624553   0.99956285   1.89942296]\n",
      " [ 59.           0.52624553   0.99956285   1.89942296]\n",
      " [ 60.           0.52624553   0.99956285   1.89942296]\n",
      " [ 61.           0.52624553   0.99956285   1.89942296]\n",
      " [ 62.           0.52624553   0.99956285   1.89942296]\n",
      " [ 63.           0.52624553   0.99956285   1.89942296]\n",
      " [ 64.           0.52624553   0.99956285   1.89942296]\n",
      " [ 65.           0.52624553   0.99956285   1.89942296]\n",
      " [ 66.           0.52624553   0.99956285   1.89942296]\n",
      " [ 67.           0.52624553   0.99956285   1.89942296]\n",
      " [ 68.           0.52624553   0.99956285   1.89942296]\n",
      " [ 69.           0.52624553   0.99956285   1.89942296]\n",
      " [ 70.           0.52624553   0.99956285   1.89942296]\n",
      " [ 71.           0.52624553   0.99956285   1.89942296]\n",
      " [ 72.           0.52624553   0.99956285   1.89942296]\n",
      " [ 73.           0.52624553   0.99956285   1.89942296]\n",
      " [ 74.           0.52624553   0.99956285   1.89942296]\n",
      " [ 75.           0.52624553   0.99956285   1.89942296]\n",
      " [ 76.           0.52624553   0.99956285   1.89942296]\n",
      " [ 77.           0.52624553   0.99956285   1.89942296]\n",
      " [ 78.           0.52624553   0.99956285   1.89942296]\n",
      " [ 79.           0.52624553   0.99956285   1.89942296]\n",
      " [ 80.           0.52624553   0.99956285   1.89942296]\n",
      " [ 81.           0.52624553   0.99956285   1.89942296]\n",
      " [ 82.           0.52624553   0.99956285   1.89942296]\n",
      " [ 83.           0.52624553   0.99956285   1.89942296]\n",
      " [ 84.           0.52624553   0.99956285   1.89942296]\n",
      " [ 85.           0.52624553   0.99956285   1.89942296]\n",
      " [ 86.           0.52624553   0.99956285   1.89942296]\n",
      " [ 87.           0.52624553   0.99956285   1.89942296]\n",
      " [ 88.           0.52624553   0.99956285   1.89942296]\n",
      " [ 89.           0.52624553   0.99956285   1.89942296]\n",
      " [ 90.           0.52624553   0.99956285   1.89942296]\n",
      " [ 91.           0.52624553   0.99956285   1.89942296]\n",
      " [ 92.           0.52624553   0.99956285   1.89942296]\n",
      " [ 93.           0.52624553   0.99956285   1.89942296]\n",
      " [ 94.           0.52624553   0.99956285   1.89942296]\n",
      " [ 95.           0.52624553   0.99956285   1.89942296]\n",
      " [ 96.           0.52624553   0.99956285   1.89942296]\n",
      " [ 97.           0.52624553   0.99956285   1.89942296]\n",
      " [ 98.           0.52624553   0.99956285   1.89942296]\n",
      " [ 99.           0.52624553   0.99956285   1.89942296]\n",
      " [100.           0.52624553   0.99956285   1.89942296]]\n"
     ]
    }
   ],
   "source": [
    "print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "scatter",
         "uid": "a7853bdc-a9f2-11e8-a105-e03f49bb7a31",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100
         ],
         "y": [
          1.9000407016383851,
          1.8994290328691226,
          1.899418604675145,
          1.899433237470065,
          1.899423420745524,
          1.8994218596252446,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264,
          1.899422964853264
         ]
        }
       ],
       "layout": {
        "title": "hello world"
       }
      },
      "text/html": [
       "<div id=\"916d3252-cacd-4f34-81d6-d8afb6183f15\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"916d3252-cacd-4f34-81d6-d8afb6183f15\", [{\"x\": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0], \"y\": [1.9000407016383851, 1.8994290328691226, 1.899418604675145, 1.899433237470065, 1.899423420745524, 1.8994218596252446, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264], \"type\": \"scatter\", \"uid\": \"a78959e2-a9f2-11e8-8f34-e03f49bb7a31\"}], {\"title\": \"hello world\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"916d3252-cacd-4f34-81d6-d8afb6183f15\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"916d3252-cacd-4f34-81d6-d8afb6183f15\", [{\"x\": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0], \"y\": [1.9000407016383851, 1.8994290328691226, 1.899418604675145, 1.899433237470065, 1.899423420745524, 1.8994218596252446, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264, 1.899422964853264], \"type\": \"scatter\", \"uid\": \"a78959e2-a9f2-11e8-8f34-e03f49bb7a31\"}], {\"title\": \"hello world\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "plotly.offline.iplot({\n",
    "    \"data\": [go.Scatter(x=performance[:,0], y=performance[:,3])],\n",
    "    \"layout\": go.Layout(title=\"hello world\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pennsylvania', 'grand', 'jury', 'report', 'faults', 'cardinal', 'donald', 'wuerl,', 'the', 'former', 'longtime', 'bishop', 'of', 'pittsburgh,', 'over', 'his', 'handling', 'of', 'allegedly']\n",
      "['september', '17,', 'only', 'one', 'dog', 'or', 'cat—either', 'in', 'a', 'carrier', 'or', 'on', 'a', 'leash—will', 'be', 'allowed', 'per', 'customer', 'on']\n",
      "['rico', 'electric', 'power', 'authority', 'said', 'tuesday', 'that', 'today', 'represents', 'the', 'end', 'of', 'the', 'restoration', 'of', 'power', 'to', 'customers', 'that']\n",
      "['bishop', 'of', 'harrisburg,', 'pennsylvania,', 'held', 'a', 'press', 'conference', 'wednesday', 'endofsen', 'coming', 'up', 'in', 'the', 'next', '{{countdown}}', '{{countdownlbl}}', 'endofpar', 'coming']\n",
      "['pennsylvania', 'grand', 'jury', 'investigated', 'clergy', 'sexual', 'abuse', 'of', 'children', 'endofsen', '\\n', 'endofpar', 'hundreds', 'of', 'roman', 'catholic', 'priests', 'in', 'pennsylvania']\n",
      "['grand', 'jury', 'report', 'found', 'top', 'pennsylvania', 'catholic', 'officials', 'covered', 'up', 'child', 'sex', 'abuse', 'for', '70', 'years', 'endofsen', 'endofpar', 'share']\n",
      "['handful', 'of', 'democratic', 'lawmakers', 'have', 'some', 'questions', 'for', 'fcc', 'chairman', 'ajit', 'pai', 'regarding', 'claims', 'of', 'a', 'ddos', 'attack', 'that']\n",
      "['breaking', 'national', 'and', 'world', 'news,', 'broadcast', 'video', 'coverage,', 'and', 'exclusive', 'interviews', 'endofsen', 'find', 'the', 'top', 'news', 'online', 'at', 'abc']\n",
      "['breaking', 'national', 'and', 'world', 'news,', 'broadcast', 'video', 'coverage,', 'and', 'exclusive', 'interviews', 'endofsen', 'find', 'the', 'top', 'news', 'online', 'at', 'abc']\n",
      "['has', 'hired', 'a', 'new', 'chief', 'security', 'officer,', 'the', 'new', 'york', 'times', 'reports,', 'filling', 'the', 'role', 'that', 'has', 'remained', 'vacant']\n",
      "['campaign', 'is', 'seeking', 'millions', 'from', 'the', 'former', 'white', 'house', 'aide', 'endofsen', 'coming', 'up', 'in', 'the', 'next', '{{countdown}}', '{{countdownlbl}}', 'endofpar']\n",
      "['breaking', 'national', 'and', 'world', 'news,', 'broadcast', 'video', 'coverage,', 'and', 'exclusive', 'interviews', 'endofsen', 'find', 'the', 'top', 'news', 'online', 'at', 'abc']\n",
      "['new', 'york', 'judge,', 'citing', 'the', 'long', 'history', 'of', 'the', 'casting', 'couch', 'in', 'hollywood,', 'says', 'a', 'civil', 'lawsuit', 'against', 'harvey']\n",
      "['muir', 'believes', 'her', 'european', 'championships', 'success', 'proves', '\"you', 'really', 'can', 'do', 'it\"', 'at', 'international', 'level', 'while', 'being', 'based', 'in']\n",
      "['chinese', 'tree', 'has', 'flowered', 'for', 'the', 'first', 'time', 'since', 'it', 'was', 'planted', 'in', 'cardiff', 'more', 'than', '100', 'years', 'ago']\n",
      "['singh,', '37,', 'attacked', 'his', 'victim', 'at', 'his', 'flat', 'in', 'govan,', 'glasgow,', 'last', 'april', 'after', 'a', 'night', 'out', 'endofsen', 'endofpar']\n",
      "['labour', 'leader', 'says', 'he', 'was', 'at', 'an', 'event', 'in', '2014', 'in', 'order', 'to', 'promote', 'peace', 'in', 'the', 'middle', 'east']\n",
      "['news', 'settlement', 'in', 'kayak', 'drowning', 'case;', 'fiancee', 'did', 'prison', 'time', 'abc', 'news', 'a', 'lawsuit', 'settlement', 'has', 'been', 'reached', 'in']\n",
      "['european', 'union', 'is', 'increasing', 'pressure', 'on', 'poland', 'over', 'what', 'it', 'sees', 'as', 'flaws', 'in', 'the', 'country', 'supreme', 'court', 'law']\n",
      "['sprinter', 'dina', 'asher-smith', 'recalls', 'her', 'very', 'first', 'race', 'when', 'she', 'was', 'eight', 'endofsen', 'endofpar', 'share', 'this', 'with', 'endofpar', 'email']\n",
      "['britain', 'dina', 'asher-smith', 'says', 'she', 'will', 'have', 'to', '\"get', 'a', 'little', 'bit', 'faster\"', 'to', 'challenge', 'for', 'golds', 'at', 'the']\n",
      "['edwards', 'was', 'previously', 'found', 'guilty', 'of', 'attempted', 'murder', 'after', 'the', 'shootings', 'in', 'hamilton', 'in', '2016', 'endofsen', 'endofpar', 'share', 'this']\n",
      "['63-year-old', 'was', 'tag', 'team', 'partner', 'with', 'his', 'brother-in-law', 'bret', 'hart', 'endofsen', 'his', 'daughter', 'is', 'also', 'a', 'wrestler', 'endofsen', 'endofpar']\n",
      "['that', 'a', 'chinese', 'company', 'had', 'been', 'contracted', 'to', 'print', 'indian', 'notes', 'triggered', 'a', 'backlash', 'online', 'endofsen', 'endofpar', 'share', 'this']\n",
      "['is', 'blaming', '\"anti-china', 'forces\"', 'for', 'the', 'growing', 'criticism', 'of', 'beijing', 'policies', 'in', 'a', 'far', 'western', 'region', 'where', 'large', 'groups']\n",
      "['most', 'senior', 'roman', 'catholic', 'cleric', 'convicted', 'of', 'covering', 'up', 'child', 'sex', 'abuse', 'has', 'been', 'ordered', 'by', 'an', 'australian', 'court']\n",
      "['wilson,', 'the', 'former', 'archbishop', 'of', 'adelaide,', 'covered', 'up', 'sexual', 'abuse', 'committed', 'in', 'the', '1970s', 'endofsen', 'endofpar', 'share', 'this', 'with']\n",
      "['battery-powered', 'trucks', 'to', 'formula', 'e', 'racing', 'cars,', 'electric', 'vehicles', 'are', 'going', 'mainstream', 'around', 'the', 'world', 'endofsen', 'endofpar', 'share', 'this']\n",
      "['uzbek', 'refugee', 'serving', '25', 'years', 'in', 'prison', 'for', 'plotting', 'to', 'helping', 'a', 'foreign', 'terrorist', 'group', 'has', 'been', 'sentenced', 'to']\n",
      "['claim', 'six', 'medals', 'on', 'day', 'one', 'of', 'the', 'world', 'para-swimming', 'european', 'championships', 'in', 'dublin,', 'with', 'scott', 'quin', 'winning', 'gold']\n",
      "['fisheries', 'minister', 'broke', 'security', 'rules', 'on', 'a', 'trip', 'to', 'iran', 'with', 'his', 'ex-beauty', 'queen', 'girlfriend', 'endofsen', 'endofpar', 'share', 'this']\n",
      "['rescued', '60', 'cats', 'and', 'dogs', 'from', 'an', 'animal', 'shelter', 'threatened', 'by', 'a', 'wildfire', 'endofsen', 'endofpar', 'share', 'this', 'with', 'endofpar']\n",
      "['worboys', 'has', 'been', 'questioned', 'about', 'a', 'number', 'of', 'new', 'allegations', 'of', 'sexual', 'assault', 'endofsen', 'endofpar', 'share', 'this', 'with', 'endofpar']\n",
      "['police', 'spokesman', 'in', 'eastern', 'poland', 'says', 'an', 'israeli', 'teen', 'has', 'admitted', 'to', 'dropping', 'his', 'pants', 'at', 'the', 'former', 'nazi']\n",
      "['prominent', 'jewish-american', 'commentator', 'who', 'has', 'been', 'critical', 'of', 'israel', 'says', 'he', 'was', 'detained', 'by', 'israeli', 'airport', 'authorities', 'and', 'interrogated']\n",
      "['labour', 'leader', 'responds', 'to', 'criticism', 'of', 'his', 'presence', 'at', 'a', '2014', 'wreath-laying', 'ceremony', 'in', 'tunisia', 'endofsen', 'endofpar', 'share', 'this']\n",
      "['opposition', 'leader', 'jeremy', 'corbyn,', 'facing', 'allegations', 'of', 'enabling', 'anti-semitism,', 'has', 'acknowledged', 'he', 'was', 'present', 'at', 'a', 'wreath-laying', 'to', 'palestinians']\n"
     ]
    }
   ],
   "source": [
    "k=4\n",
    "for i in range(articleCentroidIds.shape[0]):\n",
    "    if articleCentroidIds[i]==k:\n",
    "        print(articles[i][1:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trouble is, this doesn't work very well...\n",
    "\n",
    "Could be to do with the size of the vectors...\n",
    "\n",
    "plot cluster quality with generations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "Superficial text vs deeper meaning\n",
    "\n",
    "Text frequency vectors imply all words are distinct and orthogonal, which is not true. Tunguska is strongly related to the words Incident and Meteor, despite being superficially dissimilar to both\n",
    "\n",
    "This is due to **context** and **understanding** of the words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
