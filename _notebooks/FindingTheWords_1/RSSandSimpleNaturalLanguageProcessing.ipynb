{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(c(\"stringdist\",\"feedeR\",\"foreach\",\"doParallel\",\"rvest\"))\n",
    "\n",
    "library(doParallel)\n",
    "cl <- makeCluster(4)\n",
    "registerDoParallel(cl)\n",
    "library(foreach)\n",
    "\n",
    "\n",
    "#feed titles\n",
    "library(feedeR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: foreach\n",
      "Loading required package: iterators\n",
      "Loading required package: parallel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space required after the Public Identifier\n",
      "SystemLiteral \" or ' expected\n",
      "SYSTEM or PUBLIC, the URI is missing\n",
      "Space required after the Public Identifier\n",
      "SystemLiteral \" or ' expected\n",
      "SYSTEM or PUBLIC, the URI is missing\n",
      "Space required after the Public Identifier\n",
      "SystemLiteral \" or ' expected\n",
      "SYSTEM or PUBLIC, the URI is missing\n",
      "Space required after the Public Identifier\n",
      "SystemLiteral \" or ' expected\n",
      "SYSTEM or PUBLIC, the URI is missing\n",
      "Space required after the Public Identifier\n",
      "SystemLiteral \" or ' expected\n",
      "SYSTEM or PUBLIC, the URI is missing\n",
      "Space required after the Public Identifier\n",
      "SystemLiteral \" or ' expected\n",
      "SYSTEM or PUBLIC, the URI is missing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## GATHER RAW DATA\n",
    "\n",
    "feeds <- c(\"http://feeds.bbci.co.uk/news/world/rss.xml\",\n",
    "            \"http://feeds.bbci.co.uk/news/rss.xml\",\n",
    "            \"http://feeds.skynews.com/feeds/rss/uk.xml\",\n",
    "            \"http://feeds.skynews.com/feeds/rss/world.xml\",\n",
    "            \"http://feeds.skynews.com/feeds/rss/us.xml\",\n",
    "            \"http://feeds.reuters.com/Reuters/domesticNews\",\n",
    "            \"http://feeds.reuters.com/Reuters/worldNews\",\n",
    "            \"http://feeds.foxnews.com/foxnews/national\",\n",
    "            \"http://feeds.foxnews.com/foxnews/world\",\n",
    "            \"http://rssfeeds.usatoday.com/UsatodaycomWorld-TopStories\",\n",
    "            \"http://rssfeeds.usatoday.com/UsatodaycomNation-TopStories\",\n",
    "            \"http://rss.nytimes.com/services/xml/rss/nyt/World.xml\",\n",
    "            \"http://www.nytimes.com/services/xml/rss/nyt/Africa.xml\",\n",
    "            \"http://www.nytimes.com/services/xml/rss/nyt/Americas.xml\",\n",
    "            \"http://www.nytimes.com/services/xml/rss/nyt/AsiaPacific.xml\",\n",
    "            \"http://www.nytimes.com/services/xml/rss/nyt/Europe.xml\",\n",
    "            \"http://www.nytimes.com/services/xml/rss/nyt/MiddleEast.xml\",\n",
    "            \"http://www.nytimes.com/services/xml/rss/nyt/US.xml\",\n",
    "            \"http://www.telegraph.co.uk/news/rss.xml\"\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "dataset <- list()\n",
    "for (i in 1:length(feeds)) {\n",
    "    dataset <- tryCatch({\n",
    "    extract <- feed.extract(feeds[i])\n",
    "    extract <- cbind.data.frame(feed = extract$title, extract$items)\n",
    "    rbind.data.frame(dataset, extract)\n",
    "    }, error = function(e) { dataset })\n",
    "}\n",
    "dataset <- dataset[match(unique(dataset$link), dataset$link),]\n",
    "titles <- sapply(dataset$title, function(s) {(strsplit(gsub(\"[[:punct:]]\", \" \", tolower(s)), \" \")) })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "300"
      ],
      "text/latex": [
       "300"
      ],
      "text/markdown": [
       "300"
      ],
      "text/plain": [
       "[1] 300"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nrow(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#feed contents\n",
    "#library(rvest)\n",
    "\n",
    "contents <- foreach(i = 1:nrow(dataset), .combine = rbind, .packages = \"rvest\") %dopar% {\n",
    "    #i <- 10\n",
    "    tryCatch({\n",
    "        page <- read_html(dataset$link[i])\n",
    "        paragraphs <- html_nodes(page, \"p\")\n",
    "        p_classes <- html_attr(paragraphs, \"class\")\n",
    "        p_text <- html_text(paragraphs)\n",
    "        words <- as.character(unlist(sapply(p_text[is.na(p_classes) | grepl(\"intro\", tolower(p_classes))],\n",
    "                                            function(s) { strsplit(s, \" \")})))\n",
    "        out <- sapply(words[1:100], function(s) { tolower(gsub(\"[^[:alnum:][:space:]]\", \"\", s)) })\n",
    "        names(out) <- NULL\n",
    "        #c(hash = dataset$hash[i], out)\n",
    "        c(id=i, out)\n",
    "    }, error = function(e) { \"\" })\n",
    "}\n",
    "#contents[1,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "300"
      ],
      "text/latex": [
       "300"
      ],
      "text/markdown": [
       "300"
      ],
      "text/plain": [
       "[1] 300"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nrow(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## VECTORISE DATA\n",
    "\n",
    "#create wordvector\n",
    "contentWords <- table(c(contents))\n",
    "contentWords <- cbind.data.frame(word = tolower(names(contentWords)[1:length(names(contentWords))]),\n",
    "                                 count = contentWords)\n",
    "\n",
    "wordVector <- unique(c(tolower(unlist(titles)),\n",
    "                       tolower(unlist(contents))))\n",
    "wordVector <- wordVector[wordVector != \"\"]\n",
    "\n",
    "\n",
    "vectorisedData <- foreach(i = 1:nrow(dataset), .combine=rbind) %dopar% {\n",
    "    #i <- 1\n",
    "    tVector <- integer(length(wordVector))\n",
    "    row <- contents[contents[, \"id\"] == i,]\n",
    "    countVector <- table(c(titles[[i]], row[2:length(row)]))\n",
    "    for (j in 1:length(countVector)) {\n",
    "        index <- match(names(countVector)[j], wordVector)\n",
    "        tVector[index] <- countVector[j]\n",
    "    }\n",
    "    t(tVector)\n",
    "    #vectorisedData <- rbind.data.frame(vectorisedData, t(as.data.frame(tVector))) #tVector\n",
    "}\n",
    "names(vectorisedData) <- wordVector\n",
    "\n",
    "vectorisedData <- as.matrix(vectorisedData)\n",
    "colnames(vectorisedData) <- wordVector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in cor(vectorisedData):\n",
      "\"the standard deviation is zero\""
     ]
    }
   ],
   "source": [
    "\n",
    "## Word Bundles\n",
    "correlationmatrix <- cor(vectorisedData)\n",
    "bundles <- as.data.frame(foreach(i = 1:ncol(vectorisedData), .combine = rbind) %do% {\n",
    "    #i<-1\n",
    "    vec <- correlationmatrix[, i]\n",
    "    bundle <- vec[vec > 0.9]\n",
    "    c(id = i,\n",
    "      bundle = paste(names(bundle[!is.na(bundle)]), collapse = \"-\"),\n",
    "      value = sum(bundle[!is.na(bundle)]),\n",
    "      first = colnames(correlationmatrix)[i],\n",
    "      length = length(bundle[!is.na(bundle)]))\n",
    "})\n",
    "bundles <- bundles[match(unique(bundles$bundle[as.numeric(bundles$value) > 1]), bundles$bundle),]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Synonyms\n",
    "library(stringdist)\n",
    "similaritymatrix <- matrix(foreach(i = 1:length(wordVector), .combine = rbind, .packages = \"stringdist\") %dopar% { stringdist(wordVector[i], wordVector) },\n",
    "                           ncol = length(wordVector),\n",
    "                           nrow = length(wordVector),\n",
    "                           dimnames = list(wordVector,wordVector))\n",
    "synonyms <- as.data.frame(foreach(i = 1:ncol(vectorisedData), .combine = rbind) %do% {\n",
    "    #i<-1\n",
    "    vec <- similaritymatrix[, i]\n",
    "    synonym <- vec[vec < 0.25 * length(colnames(similaritymatrix)[i])]\n",
    "    c(id = i,\n",
    "      synonym = paste(names(synonym[!is.na(synonym)]), collapse = \"-\"),\n",
    "      value = sum(synonym[!is.na(synonym)]))\n",
    "})\n",
    "synonyms <- synonyms[match(unique(synonyms$synonym[as.numeric(synonyms$value) > 1]), synonyms$synonym),]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"top_n\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"top_n\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## SELECT FEATURES\n",
    "analytics <- cbind.data.frame(wordVector,\n",
    "                              count = sapply(1:length(wordVector), function(w) { sum(vectorisedData[, w]) }),\n",
    "                              mean = sapply(1:length(wordVector), function(w) { mean(vectorisedData[, w]) }),\n",
    "                              stdev = sapply(1:length(wordVector), function(w) { sd(vectorisedData[, w]) }),\n",
    "                              max = sapply(1:length(wordVector), function(w) { max(vectorisedData[, w]) }),\n",
    "                              min = sapply(1:length(wordVector), function(w) { min(vectorisedData[, w]) }))\n",
    "\n",
    "analytics$varration <- sapply(1:nrow(analytics), function(a) { analytics$stdev[a] / analytics$mean[a] })\n",
    "\n",
    "test <- top_n(analytics, 50, analytics$count)\n",
    "test <- analytics[rank(analytics$count, ties.method = \"random\"),]\n",
    "\n",
    "lengths <- sapply(1:nrow(vectorisedData), function(i) { sum(vectorisedData[i,]) })\n",
    "lengths <- cbind.data.frame(length = lengths)\n",
    "\n",
    "\n",
    "library(dplyr)\n",
    "#analytics[sort(analytics$count, decreasing = T),]\n",
    "#selectedFeatures <- as.character(analytics[!is.na(analytics$varration) & analytics$varration > 2,]$wordVector)\n",
    "#selectedFeatures <- as.character(top_n(analytics, 200, analytics$varration)$wordVector)\n",
    "#selectedFeatures <- names(vectorisedData)\n",
    "selectedFeatures <- as.character(bundles$first[as.numeric(bundles$length)>2])\n",
    "\n",
    "featureSet <- matrix(as.numeric(vectorisedData[, selectedFeatures]),nrow = nrow(vectorisedData))#,\n",
    "                               #date = with(dataset, (as.numeric(date) - quantile(as.numeric(date), 0.05)) / (mean(as.numeric(date)) - quantile(as.numeric(date), 0.05))),\n",
    "                               #hash_ = dataset$hash)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## K MEANS CLUSTERING\n",
    "cartesianDistance <- function(v1, v2) {\n",
    "    #sum(mapply(function(c1, c2) {(c1 - c2) ^ 2 }, v1, v2))^0.5\n",
    "    ((v1 - v2) %*% t(v1 - v2))[[1]]^0.5\n",
    "}\n",
    "\n",
    "generateCentroid <- function() {\n",
    "    centroid <- double(length(selectedFeatures))\n",
    "    #centroid[sample(1:length(selectedFeatures), round(mean(lengths$length) + 1))] <- 1\n",
    "    centroid <- as.numeric(featureSet[sample(1:nrow(dataset),1),])\n",
    "    centroid\n",
    "}\n",
    "\n",
    "k <- 10\n",
    "centroids <- list()\n",
    "for (i in 1:k) {\n",
    "    centroids <- rbind.data.frame(centroids, generateCentroid())\n",
    "}\n",
    "centroids <- as.matrix(centroids)\n",
    "colnames(centroids)<- NULL  \n",
    "\n",
    "G <- 100\n",
    "history <- list()\n",
    "g <- 1\n",
    "while (g <= G) {\n",
    "    dataset$cluster <- foreach(i = 1:nrow(featureSet), .combine = rbind) %dopar% {\n",
    "        #i <- 1\n",
    "        distances <- sapply(1:k, function(c) {\n",
    "            #c <- 2\n",
    "            V1 <- as.matrix(centroids[c,])\n",
    "            V2 <- as.matrix(featureSet[i, ])\n",
    "            (t(V1-V2) %*% (V1-V2))[[1]]^0.5\n",
    "        })\n",
    "        match(min(distances), distances)\n",
    "    }\n",
    "\n",
    "    newcentroids <- centroids\n",
    "    for (i in 1:k) {\n",
    "        #i <- 2\n",
    "        if (nrow(dataset[dataset$cluster == i, ]) > 1) {\n",
    "            #i <- 1\n",
    "            members <- as.data.frame(featureSet[dataset$cluster == i,])\n",
    "            for (j in 1:length(selectedFeatures)) {\n",
    "                newcentroids[i, j] = mean(members[, j])\n",
    "            }\n",
    "        } else {\n",
    "            newcentroid <- generateCentroid()\n",
    "            for (j in 1:length(selectedFeatures)) {\n",
    "                newcentroids[i, j] = newcentroid[j]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    lastCentroids <- as.matrix(centroids)\n",
    "    centroids <- as.matrix(newcentroids)\n",
    "\n",
    "    dataset$distance <- foreach(i = 1:nrow(featureSet), .combine = rbind) %dopar% {\n",
    "        # i <- 1\n",
    "        V1 <- as.matrix(centroids[dataset$cluster[i],]) #, nrow = 1)\n",
    "        V2 <- as.matrix(featureSet[i,])\n",
    "        (t(V1 - V2) %*% (V1 - V2))[[1]] ^ 0.5\n",
    "    }\n",
    "                                     \n",
    "    distances <- double(k)\n",
    "    for (i in 1:k) {\n",
    "        distances[i] <- sum(dataset$distance[dataset$cluster == i]) / length(dataset$distance[dataset$cluster == i])\n",
    "    }\n",
    "    history <- rbind.data.frame(history, cbind(g, t(distances), sum(distances)))\n",
    "\n",
    "    g <- g + 1\n",
    "}\n",
    "names(history) <- c(\"generation\", 1:k, \"total\")\n",
    "View(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ANALYSIS\n",
    "\n",
    "test<-dataset[dataset$cluster==1,]\n",
    "View(test)\n",
    "\n",
    "test <- dataset[order(dataset$cluster, dataset$distance),]\n",
    "View(test)\n",
    "\n",
    "test2 <- merge(x = featureSet[, names(featureSet)[!names(featureSet) %in% selectedFeatures]],\n",
    "               y = dataset,\n",
    "               by.x = \"hash_\",\n",
    "               by.y = \"hash\")\n",
    "\n",
    "history\n",
    "\n",
    "cl <- 9\n",
    "result <- top_n(test2[test2$clusters_ == cl,], 10, test2$distance_[test2$clusters_ == cl])\n",
    "View(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
